{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expirement\n",
    "\n",
    "KMeans clustering using MASE joint embedding (FA_R2, MD_R2, IS_MDF)\n",
    " \n",
    "##### Determine number clusters per bundle based on population data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE `662551` not in `HCP_1200`**\n",
    "\n",
    "```\n",
    "s3://profile-hcp-west/hcp_reliability/single_shell/hcp_1200_afq/\n",
    "s3://profile-hcp-west/hcp_reliability/single_shell/hcp_1200_afq_CSD/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = ['MASE_FA_Sklearn_KMeans']\n",
    "\n",
    "subjects = [\n",
    "    '103818', '105923', '111312', '114823', '115320',\n",
    "    '122317', '125525', '130518', '135528', '137128',\n",
    "    '139839', '143325', '144226', '146129', '149337',\n",
    "    '149741', '151526', '158035', '169343', '172332',\n",
    "    '175439', '177746', '185442', '187547', '192439',\n",
    "    '194140', '195041', '200109', '200614', '204521',\n",
    "    '250427', '287248', '341834', '433839', '562345',\n",
    "    '599671', '601127', '627549', '660951', # '662551', \n",
    "    '783462', '859671', '861456', '877168', '917255'\n",
    "]\n",
    "\n",
    "# session_names = ['HCP_1200']\n",
    "session_names = ['HCP_1200', 'HCP_Retest']\n",
    "\n",
    "# bundle_names = ['SLF_L']\n",
    "# bundle_names = ['SLF_L', 'SLF_R']\n",
    "bundle_names = ['ARC_L', 'ARC_R']\n",
    "# bundle_names = ['SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'CST_L', 'CST_R']\n",
    "# bundle_names = [\n",
    "#     'ATR_L', 'ATR_R',\n",
    "#     'CGC_L', 'CGC_R',\n",
    "#     'CST_L', 'CST_R',\n",
    "#     'IFO_L', 'IFO_R',\n",
    "#     'ILF_L', 'ILF_R',\n",
    "#     'SLF_L', 'SLF_R',\n",
    "#     'ARC_L', 'ARC_R',\n",
    "#     'UNC_L', 'UNC_R',\n",
    "#     'FA', 'FP'\n",
    "# ]\n",
    "\n",
    "scalars = [['DTI_FA']]\n",
    "\n",
    "# range_n_clusters = range(2,10)\n",
    "range_n_clusters = [2, 3, 4] # choosing minimal bundles and maximum to get silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# args = list(itertools.product(subjects, session_names, bundle_names))\n",
    "args = list(itertools.product(experiment_names, subjects, session_names, bundle_names, range_n_clusters, scalars))\n",
    "args"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "args = args[:len(range_n_clusters)]\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful for when individual jobs fail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def job_args(ids):\n",
    "    import numpy as np\n",
    "    \n",
    "    return list(map(tuple, np.array(args)[ids].tolist()))\n",
    "    \n",
    "# TODO use knot to determine failed and running jobs\n",
    "# may want to stop running and rerun these with larger memory and disk allocations\n",
    "def failed_jobs():\n",
    "    import numpy as np\n",
    "    # failed_job_ids = [19,22,81,101,148,149]\n",
    "    failed_job_ids = [19,22,81,101]\n",
    "#     print(len(failed_job_ids))\n",
    "    return job_args(np.array(failed_job_ids)+8)\n",
    "\n",
    "def running_jobs():\n",
    "    import numpy as np\n",
    "    running_job_ids = [1,3,13,15,35,59,83,117,119,131,137,139,141,143,147,163]\n",
    "#     print(len(running_job_ids))\n",
    "    return job_args(np.array(running_job_ids)+8)\n",
    "\n",
    "def rerun_targets():\n",
    "    import numpy as np\n",
    "    # failed twice\n",
    "    targets = list(map(tuple, np.array([*failed_jobs(), *running_jobs()])[[0,5,6,7,10,12]]))\n",
    "    return targets\n",
    "\n",
    "# print([*failed_jobs(), *running_jobs()])\n",
    "# print(rerun_targets())\n",
    "print(job_args([14,15,62,63]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all-in-one: `subbundle3`, `subbundle4`, and `subbundle5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Adjacency Spectral Emdedding (MASE)** \n",
    "\n",
    "- UNWARPED **Fractional Anisotropy Coefficient of Determination (FA R2)** \n",
    "\n",
    "- UNWARPED **Mean Diffusivity Coefficient of Determination (MD R2)** \n",
    "\n",
    "- **Inverse Scaled Minimum average Direct-Flip (IS MDF) distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subbundle(experiment_name, subject, session, bundle_name, n_clusters, scalars, clean_bundles=True):\n",
    "    \"\"\"\n",
    "    Run clustering for K=`n_clusters` on `subject`, `session`, `bundle_name` \n",
    "    using `clean_bundles` CSD tractography\n",
    "    \n",
    "    Features:\n",
    "    - Scalars\n",
    "      - Tissue - Fractional Anisotropy Coefficient of Determination (FA R^2) \n",
    "      - Tissue - Mean Diffusivity Coefficient of Determination (MD R^2)\n",
    "    - Distance - Inverse Scaled Minimum average Direct-Flip distance (IS_MDF)\n",
    "    \n",
    "    Generates:\n",
    "    - Joint graph adjacency spectral embeddings using `MASE`\n",
    "    - Clusters using `KMeans`\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from os.path import exists\n",
    "    import s3fs\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    from dipy.io.streamline import load_tractogram, save_tractogram\n",
    "    from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "    from dipy.io.utils import create_nifti_header, get_reference_info\n",
    "    from dipy.tracking.streamline import set_number_of_points, values_from_volume, bundles_distances_mdf\n",
    "    from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "    from AFQ.segmentation import clean_bundle\n",
    "    from graspologic.embed import MultipleASE as MASE\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "    \n",
    "    logger = logging.getLogger(\"s3fs\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    def coeff_of_determination(data, model, axis=-1):\n",
    "        \"\"\"\n",
    "         http://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "                  _                                           _\n",
    "                 |    sum of the squared residuals             |\n",
    "        R^2 =    |1 - ---------------------------------------  | * 100\n",
    "                 |_   sum of the squared mean-subtracted data _|\n",
    "        \"\"\"\n",
    "        X = np.empty((data.shape[0], model.shape[0]))\n",
    "        demeaned_data = data - np.mean(data, axis=axis)[...,np.newaxis]\n",
    "        ss_tot = np.sum(demeaned_data **2, axis=axis)\n",
    "        \n",
    "        # Don't divide by 0:\n",
    "        if np.all(ss_tot==0.0):\n",
    "            X[:, :] = np.nan\n",
    "            return X\n",
    "        \n",
    "        for ii in range(X.shape[0]):\n",
    "            for jj in range(X.shape[1]):\n",
    "                # There's no point in doing any of this: \n",
    "                if np.all(data[ii]==0.0) and np.all(model[ii]==0.0):\n",
    "                    X[ii, jj] = np.nan\n",
    "                else:\n",
    "                    residuals = data[ii] - model[jj]\n",
    "                    ss_err = np.sum(residuals ** 2, axis=axis)\n",
    "                    X[ii, jj] = 1 - (ss_err/ss_tot[ii])\n",
    "        return X\n",
    "\n",
    "    def rrss(y, yhat):\n",
    "        \"\"\"\n",
    "        Compute root residual sum of squares\n",
    "        \"\"\"\n",
    "        residuals = y - yhat\n",
    "        rss = np.dot(residuals.T, residuals)\n",
    "        rrss = np.sqrt(rss)\n",
    "        \n",
    "        return rrss\n",
    "\n",
    "    def relabel_clusters(cluster_labels):\n",
    "        \"\"\"\n",
    "        Arrange cluster labels by number of streamlines\n",
    "        \"\"\"\n",
    "        from_values = np.flip(np.argsort(np.bincount(cluster_labels))[-(np.unique(cluster_labels).size):])\n",
    "        to_values = np.arange(from_values.size)\n",
    "\n",
    "        d = dict(zip(from_values, to_values))\n",
    "\n",
    "        new_cluster_labels = np.copy(cluster_labels)\n",
    "\n",
    "        for k, v in d.items():\n",
    "            new_cluster_labels[cluster_labels == k] = v\n",
    "\n",
    "        return new_cluster_labels\n",
    "        \n",
    "    def cluster_tractograms(model_prefix, bundle_tractogram, cluster_labels, filtered_cluster_labels, filtred_bundle_tractogram):\n",
    "        \"\"\"\n",
    "        For each cluster create `cluster_tractogram`s\n",
    "        \"\"\"\n",
    "        \n",
    "        for cluster_label in np.unique(cluster_labels):\n",
    "            #  save `model_cluster_tractogram` using `cluster_labels`\n",
    "            f_name = f'{model_prefix}_cluster_{cluster_label}_model.trk'\n",
    "            cluster_indicies = np.array(np.where(cluster_labels == cluster_label)[0])\n",
    "            model_cluster_tractogram = StatefulTractogram.from_sft(bundle_tractogram.streamlines[cluster_indicies], bundle_tractogram)\n",
    "            print(f'model cluster {cluster_label} tractogram streamlines len', len(model_cluster_tractogram), flush=True)\n",
    "            save_tractogram(model_cluster_tractogram, f_name, bbox_valid_check=False)\n",
    "            print('saving model cluster tractogram:', f_name, flush=True)\n",
    "\n",
    "        for cluster_label in np.unique(filtered_cluster_labels):\n",
    "            # save `filtered_cluster_tractogram` using `filtered_cluster_labels` \n",
    "            f_name = f'{model_prefix}_cluster_{cluster_label}_filtered.trk'\n",
    "            cluster_indicies = np.array(np.where(filtered_cluster_labels == cluster_label)[0])\n",
    "            filtered_cluster_tractogram = StatefulTractogram.from_sft(filtred_bundle_tractogram.streamlines[cluster_indicies], filtred_bundle_tractogram)\n",
    "            print(f'filtered cluster {cluster_label} tractogram streamlines len', len(filtered_cluster_tractogram), flush=True)\n",
    "            save_tractogram(filtered_cluster_tractogram, f_name, bbox_valid_check=False)\n",
    "            print('saving filtered cluster tractogram:', f_name, flush=True)\n",
    "            \n",
    "            # clean `filtered_cluster_tractogram` these are the final clusters\n",
    "            clean_cluster_tractogram = clean_bundle(filtered_cluster_tractogram)\n",
    "            sft = StatefulTractogram.from_sft(clean_cluster_tractogram.streamlines, filtred_bundle_tractogram)\n",
    "            print(f'clean cluster {cluster_label} tractogram streamlines len', len(sft), flush=True)\n",
    "            f_name = f'{model_prefix}_cluster_{cluster_label}_clean.trk'\n",
    "            print(f'saving clean cluster tractogram: {f_name}', flush=True)\n",
    "            save_tractogram(sft, f_name, bbox_valid_check=False)\n",
    "            \n",
    "            if 'DTI_FA' in scalars:\n",
    "                cluster_profile = afq_profile(\n",
    "                    fa_scalar_data,\n",
    "                    clean_cluster_tractogram.streamlines,\n",
    "                    clean_cluster_tractogram.affine,\n",
    "                    n_points=100,\n",
    "                    weights=gaussian_weights(\n",
    "                        clean_cluster_tractogram.streamlines,\n",
    "                        n_points=100\n",
    "                    )\n",
    "                )\n",
    "                f_name = f'cluster_{cluster_label}_profile_fa.npy'\n",
    "                np.save(f_name, cluster_profile)\n",
    "                print('saving:', f_name, flush=True)\n",
    "                \n",
    "            if 'DTI_MD' in scalars:\n",
    "                cluster_profile = afq_profile(\n",
    "                    md_scalar_data,\n",
    "                    clean_cluster_tractogram.streamlines,\n",
    "                    clean_cluster_tractogram.affine,\n",
    "                    n_points=100,\n",
    "                    weights=gaussian_weights(\n",
    "                        clean_cluster_tractogram.streamlines,\n",
    "                        n_points=100\n",
    "                    )\n",
    "                )\n",
    "                f_name = f'cluster_{cluster_label}_profile_md.npy'\n",
    "                np.save(f_name, cluster_profile)\n",
    "                print('saving:', f_name, flush=True)\n",
    "                        \n",
    "    def cluster(clusterer, embedding, model_prefix):\n",
    "        \"\"\"\n",
    "        Run `fit_predict`, relabel according to number of streamlines in each cluster, and \n",
    "        save classifications.\n",
    "        \n",
    "        The cluster_labels file contains a label from `0` to `n_clusters - 1` for each streamline in the\n",
    "        tractogram.\n",
    "        \"\"\"\n",
    "        cluster_labels = clusterer.fit_predict(embedding)\n",
    "        \n",
    "        # sort cluster by size, for convience, \n",
    "        # will need to resort by cross-subject similarity\n",
    "        cluster_labels = relabel_clusters(cluster_labels)\n",
    "        \n",
    "        f_name = f'{model_prefix}_cluster_labels.npy'\n",
    "        np.save(f_name, cluster_labels)\n",
    "        print('saving:', f_name, flush=True)\n",
    "        \n",
    "        return cluster_labels\n",
    "        \n",
    "        \n",
    "    def filter_streamlines(bundle_tractogram, embedding, cluster_labels, model_prefix):\n",
    "        \"\"\"\n",
    "        filter the streamlines using the silhouette score. \n",
    "        keep any streamlines above the average silhouette score.\n",
    "        \"\"\"\n",
    "        print('cleaning streamlines using average silhouette score', flush=True)\n",
    "        average_silhouette_score = silhouette_score(embedding, cluster_labels)\n",
    "        sample_silhouette_scores = silhouette_samples(embedding, cluster_labels)\n",
    "        \n",
    "        # saving the embeddings and cluster_labels for generating the silhouette plots and pair plots\n",
    "        filtered_embedding = embedding[sample_silhouette_scores > average_silhouette_score]\n",
    "        f_name = f'{model_prefix}_embeddings_filtered.npy'\n",
    "        np.save(f_name, filtered_embedding)\n",
    "        print('saving:', f_name, flush=True)\n",
    "        \n",
    "        filtered_cluster_labels = cluster_labels[sample_silhouette_scores > average_silhouette_score]\n",
    "        f_name = f'{model_prefix}_cluster_labels_filtered.npy'\n",
    "        np.save(f_name, filtered_cluster_labels)\n",
    "        print('saving:', f_name, flush=True)\n",
    "        \n",
    "        # get filtered_bundle_tractogram so can generate custer_tractrogram from the filtered_cluster_labels\n",
    "        filtred_bundle_tractogram = StatefulTractogram.from_sft(\n",
    "            bundle_tractogram.streamlines[sample_silhouette_scores > average_silhouette_score], \n",
    "            bundle_tractogram\n",
    "        )\n",
    "\n",
    "        return filtered_cluster_labels, filtred_bundle_tractogram\n",
    "\n",
    "    \n",
    "    def run_cluster_algos(bundle_tractogram, embedder_name, embedding, embedding_name):\n",
    "        \"\"\"\n",
    "        Run clustering algorithms, now only using `KMeans`.\n",
    "        \"\"\"\n",
    "        model_prefix = f'{embedder_name}_kmeans_{embedding_name}'\n",
    "        clusterer = KMeans(n_clusters)\n",
    "        cluster_labels = cluster(clusterer, embedding, model_prefix)\n",
    "                \n",
    "        filtered_cluster_labels, filtred_bundle_tractogram = filter_streamlines(\n",
    "            bundle_tractogram, embedding, cluster_labels, model_prefix\n",
    "        )\n",
    "        \n",
    "        cluster_tractograms(\n",
    "            model_prefix, bundle_tractogram, cluster_labels, filtered_cluster_labels, filtred_bundle_tractogram\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def run_embeddings(bundle_tractogram, features, feature_name, embedding_dimension=None):\n",
    "        \"\"\"\n",
    "        multiple adjacency spectral embedding (mase)\n",
    "        \"\"\"\n",
    "        print('mase', flush=True)\n",
    "\n",
    "        embedder = MASE(n_components=embedding_dimension)\n",
    "        embedder_name = 'mase'\n",
    "        \n",
    "        embedding = embedder.fit_transform(features)\n",
    "        model_prefix = f'{embedder_name}_kmeans_{feature_name}'\n",
    "        f_name = f'{model_prefix}_embeddings.npy'\n",
    "        np.save(f_name, embedding)\n",
    "        print('saving:', f_name, flush=True)\n",
    "        \n",
    "        print(embedder_name, feature_name, 'embedding dimension', embedding.shape, flush=True)\n",
    "        run_cluster_algos(bundle_tractogram, embedder_name, embedding, feature_name)\n",
    "\n",
    "    print(\"begin\", experiment_name, subject, session, bundle_name, n_clusters, scalars, flush=True)\n",
    "    \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    ### fractional anisotropy scalar file ###\n",
    "    if 'DTI_FA' in scalars:\n",
    "        fa_scalar_filename = 'FA.nii.gz'\n",
    "        print('loading FA scalar file:', fa_scalar_filename, flush=True)\n",
    "\n",
    "        if not exists(fa_scalar_filename):\n",
    "            fs.get(\n",
    "                (\n",
    "                    f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "                    f'{session.lower()}_afq_CSD/sub-{subject}/ses-01/'\n",
    "                    f'sub-{subject}_dwi_model-DTI_FA.nii.gz'\n",
    "                ),\n",
    "                f'{fa_scalar_filename}'\n",
    "            )\n",
    "\n",
    "        fa_scalar_data = nib.load(fa_scalar_filename).get_fdata()\n",
    "\n",
    "    ### mean diffusivity scalar file ###\n",
    "    if 'DTI_MD' in scalars:\n",
    "        md_scalar_filename = 'MD.nii.gz'\n",
    "        print('loading scalar file: ', md_scalar_filename, flush=True)\n",
    "\n",
    "        if not exists(md_scalar_filename):\n",
    "            fs.get(\n",
    "                (\n",
    "                    f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "                    f'{session.lower()}_afq_CSD/sub-{subject}/ses-01/'\n",
    "                    f'sub-{subject}_dwi_model-DTI_MD.nii.gz'\n",
    "                ),\n",
    "                f'{md_scalar_filename}'\n",
    "            )\n",
    "\n",
    "        md_scalar_data = nib.load(md_scalar_filename).get_fdata()\n",
    "    \n",
    "    ### single shell deterministic bundle tractography ###\n",
    "    bundle_tractogram_filename = f'{bundle_name}.trk'\n",
    "    print('loading bundle tractogram:', bundle_tractogram_filename, flush=True)\n",
    "\n",
    "    bundle_folder = 'bundles'\n",
    "    \n",
    "    if clean_bundles:\n",
    "        bundle_folder = 'clean_' + bundle_folder\n",
    "    \n",
    "    if not exists(bundle_tractogram_filename):\n",
    "        fs.get(\n",
    "            (\n",
    "                f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "                f'{session.lower()}_afq_CSD/sub-{subject}/ses-01/'\n",
    "                f'{bundle_folder}/sub-{subject}_dwi_space-RASMM_model-CSD_desc-det-afq-{bundle_name}_tractography.trk'\n",
    "            ),\n",
    "            f'{bundle_tractogram_filename}'\n",
    "        )\n",
    "    \n",
    "    bundle_tractogram = load_tractogram(bundle_tractogram_filename, 'same')\n",
    "    \n",
    "    ### bundle profile ###\n",
    "    if 'DTI_FA' in scalars:\n",
    "        bundle_profile = afq_profile(\n",
    "            fa_scalar_data,\n",
    "            bundle_tractogram.streamlines,\n",
    "            bundle_tractogram.affine,\n",
    "            n_points=100,\n",
    "            weights=gaussian_weights(\n",
    "                bundle_tractogram.streamlines,\n",
    "                n_points=100\n",
    "            )\n",
    "        )\n",
    "        f_name = 'bundle_profile_fa.npy'\n",
    "        np.save(f_name, bundle_profile)\n",
    "        print('saving:', f_name, flush=True)\n",
    "    \n",
    "    if 'DTI_MD' in scalars:\n",
    "        bundle_profile = afq_profile(\n",
    "            md_scalar_data,\n",
    "            bundle_tractogram.streamlines,\n",
    "            bundle_tractogram.affine,\n",
    "            n_points=100,\n",
    "            weights=gaussian_weights(\n",
    "                bundle_tractogram.streamlines,\n",
    "                n_points=100\n",
    "            )\n",
    "        )\n",
    "        f_name = 'bundle_profile_md.npy'\n",
    "        np.save(f_name, bundle_profile)\n",
    "        print('saving:', f_name, flush=True)\n",
    "        \n",
    "    ### streamline profiles ###\n",
    "    n_points = 100\n",
    "    \n",
    "    fgarray = set_number_of_points(bundle_tractogram.streamlines, n_points)\n",
    "    \n",
    "    if len(fgarray) == 0:\n",
    "        return\n",
    "    \n",
    "    # FA Values\n",
    "    if 'DTI_FA' in scalars:\n",
    "        fa_values = np.array(values_from_volume(fa_scalar_data, fgarray, bundle_tractogram.affine))\n",
    "        f_name = 'streamline_profile_fa.npy'\n",
    "        np.save(f_name, fa_values)\n",
    "        print('saving:', f_name, flush=True)\n",
    "\n",
    "        print('fa values:', fa_values.shape, flush=True)\n",
    "\n",
    "    # MD Values\n",
    "    if 'DTI_MD' in scalars:\n",
    "        md_values = np.array(values_from_volume(md_scalar_data, fgarray, bundle_tractogram.affine))\n",
    "        f_name = 'streamline_profile_md.npy'\n",
    "        np.save(f_name, md_values)\n",
    "        print('saving: ', f_name, flush=True)\n",
    "\n",
    "        print('md values:', md_values.shape, flush=True)\n",
    "    \n",
    "    ### Inverse Scaled MDF (Minimum average Direct-Flip) ###\n",
    "    mdf = bundles_distances_mdf(fgarray, fgarray)\n",
    "    \n",
    "    # enforce symmetry\n",
    "    mdf = (mdf + mdf.T) / 2\n",
    "    \n",
    "    # inverse scale\n",
    "    is_mdf = (mdf.max() - mdf)\n",
    "    is_mdf = is_mdf / is_mdf.max()\n",
    "\n",
    "    f_name = 'adjacency_is_mdf.npy'\n",
    "    np.save(f_name, is_mdf)\n",
    "    print('saving:', f_name, flush=True)\n",
    "    \n",
    "    print('is_mdf:', is_mdf.shape, flush=True)\n",
    "    \n",
    "    ### streamline r2 ###\n",
    "    if 'DTI_FA' in scalars:\n",
    "        # calculate FA R2\n",
    "        fa_r2 = coeff_of_determination(fa_values, fa_values)\n",
    "\n",
    "        # enforce symmetry\n",
    "        fa_r2 += fa_r2.T\n",
    "        fa_r2 = fa_r2/2\n",
    "\n",
    "        # save file\n",
    "        f_name = 'adjacency_fa_r2.npy'\n",
    "        np.save(f_name, fa_r2)\n",
    "        print('saving:', f_name, flush=True)\n",
    "\n",
    "        print('adjacency_fa_r2:', fa_r2.shape, flush=True)\n",
    "\n",
    "    if 'DTI_MD' in scalars:\n",
    "        # calculate MD R2\n",
    "        md_r2 = coeff_of_determination(md_values, md_values)\n",
    "\n",
    "        # enforce symmetry\n",
    "        md_r2 += md_r2.T\n",
    "        md_r2 = md_r2/2\n",
    "\n",
    "        # save file\n",
    "        f_name = 'adjacency_md_r2.npy'\n",
    "        np.save(f_name, md_r2)\n",
    "        print('saving: ', f_name, flush=True)\n",
    "\n",
    "        print('adjacency_md_r2:', md_r2.shape, flush=True)\n",
    "    \n",
    "    ### clustering ###\n",
    "    \n",
    "    if 'DTI_FA' in scalars:\n",
    "        fa_tissue = np.load('adjacency_fa_r2.npy')\n",
    "\n",
    "    if 'DTI_MD' in scalars:\n",
    "        md_tissue = np.load('adjacency_md_r2.npy')\n",
    "    \n",
    "    distance = np.load('adjacency_is_mdf.npy')\n",
    "    \n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    if 'DTI_FA' in scalars:\n",
    "        features.append(fa_tissue)\n",
    "        feature_names.append('fa_r2')\n",
    "        \n",
    "    if 'DTI_MD' in scalars:\n",
    "        features.append(md_tissue)\n",
    "        feature_names.append('md_r2')\n",
    "        \n",
    "    features.append(distance)\n",
    "    feature_names.append('is_mdf')\n",
    "    feature_name = '_'.join(feature_names)\n",
    "            \n",
    "    run_embeddings(bundle_tractogram, features, feature_name)\n",
    "    \n",
    "    ### upload everything to s3 ###\n",
    "    print(f'uploading to s3://hcp-subbundle/{experiment_name}/{session}/{bundle_name}/{subject}/{n_clusters}/', flush=True)\n",
    "\n",
    "    nii_files = glob.glob('*.nii.gz')\n",
    "    \n",
    "    for nii_file in nii_files:\n",
    "        fs.put(nii_file, f'hcp-subbundle/{experiment_name}/{session}/{bundle_name}/{subject}/{n_clusters}/{nii_file}')\n",
    "\n",
    "    fs.put('*.trk', f'hcp-subbundle/{experiment_name}/{session}/{bundle_name}/{subject}/{n_clusters}/')\n",
    "    fs.put('*.npy', f'hcp-subbundle/{experiment_name}/{session}/{bundle_name}/{subject}/{n_clusters}/')\n",
    "    \n",
    "    print(\"end\", experiment_name, subject, session, bundle_name, n_clusters, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test locally before running on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(delete=True):\n",
    "    \"\"\"\n",
    "    delete files in between, otherwise uploading corrupted data\n",
    "    \n",
    "    used for local testing\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    for extensions in [\"*.nii.gz\", \"*.trk\", \"*.npy\", \"*.pkl\", \"*.png\"]:\n",
    "        for file in glob.glob(extensions):\n",
    "            print('removing', file)\n",
    "            if delete:\n",
    "                os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subbundle(experiment_names[0], subjects[0], session_names[0], bundle_names[0], range_n_clusters[0], scalars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_args = list(itertools.product(experiment_names, subjects[1:], session_names, bundle_names, range_n_clusters, scalars))\n",
    "for (experiment_name, subject, session, bundle_name, n_clusters, scalars) in local_args:\n",
    "    subbundle(experiment_name, subject, session, bundle_name, n_clusters, scalars)\n",
    "    clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cloudknot as ck\n",
    "ck.set_region('us-west-2')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "knot = ck.Knot(\n",
    "    name='hcp-subbundle-' + datetime.now().isoformat()[:-7].replace(':','-'),\n",
    "    func=subbundle,\n",
    "    base_image='python:3.7',\n",
    "    image_github_installs='https://github.com/yeatmanlab/pyAFQ.git',\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    job_def_vcpus=8,\n",
    "    max_vcpus=8*len(args),\n",
    "    memory=64000,  # in MB\n",
    "    volume_size=250, # in GB\n",
    "    bid_percentage=105 # use spot instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconnect to existing knot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot = ck.Knot(name='hcp-subbundle-2021-01-11T11-56-26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse existing Docker Image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "di = ck.DockerImage(\n",
    "    name='subbundle',\n",
    "    func=subbundle,\n",
    "    base_image='ghcr.io/nrdg/pyafq:latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "knot = ck.Knot(\n",
    "    name='hcp-subbundle-' + datetime.now().isoformat()[:-7].replace(':','-'),\n",
    "    docker_image=di,\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    job_def_vcpus=8,\n",
    "    max_vcpus=8*len(args),\n",
    "    memory=64000,  # in MB\n",
    "    volume_size=250, # in GB\n",
    "    bid_percentage=105 # use spot instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run subset as trial\n",
    "# result_futures = knot.map(args[:4], starmap=True)\n",
    "\n",
    "# run all\n",
    "# result_futures = knot.map(args, starmap=True)\n",
    "\n",
    "# rerun failed\n",
    "# result_futures = knot.map(rerun_targets(), starmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.jobs[0].status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete everything associate to the knot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.clobber(clobber_pars=True, clobber_repo=True, clobber_image=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
