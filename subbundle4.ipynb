{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subbundles Part 4: Adjacencies\n",
    "\n",
    "**Subbundle** - a subgroup of streamlines with a set of common properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bloomdt/anaconda/envs/subbundles/lib/python3.8/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "import time\n",
    "import os.path as op\n",
    "\n",
    "from AFQ import api\n",
    "import AFQ.data as afd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from dipy.io.streamline import load_tractogram\n",
    "import dipy.tracking.streamline as dts\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlines (from Part 2) and Streamline Profiles (from Part 3)\n",
    "\n",
    "Streamlines are necessary to calculate mfd adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance/Adjacency Matrix of Streamline Correlations\n",
    "\n",
    "An $N \\times N$ matrix of coorelations of streamline tract profiles, where $N$ is the number of streamline profiles\n",
    "  \n",
    "  - **single matrix and single metric**\n",
    "  \n",
    "  - **single matrix and multiple metrics (weighted linear combination)**\n",
    "    \n",
    "    - manually stacked \n",
    "      \n",
    "    - learn hyperparameters\n",
    "      \n",
    "  - *multiple matrices (tensor) and multiple metrics* (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamline Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: There are multiple correlation metrics, using Pearson's r</span>\n",
    "\n",
    "- <span style=\"color:red\">**Question: are streamline profiles considered: measurement, ordinal, or categorical?**</span>\n",
    "\n",
    "  - For now stick with Pearson's r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_test_retest = False\n",
    "# compare_test_retest = True\n",
    "\n",
    "test_retest_sessions = ['test', 'retest']\n",
    "test_retest_names = ['HCP', 'HCP_retest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'HCP'\n",
    "dataset_name = 'HCP_retest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = get_subjects(dataset_name)\n",
    "# subjects = get_subjects_small(dataset_name)\n",
    "# subjects = get_subjects_medium(dataset_name)\n",
    "subjects = ['105923']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCP_retest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dipy.data.fetcher:Dataset is already in place. If you want to fetch it again please first remove the folder /Users/bloomdt/AFQ_data/templates \n",
      "INFO:dipy.data.fetcher:Dataset is already in place. If you want to fetch it again please first remove the folder /Users/bloomdt/AFQ_data/callosum_templates \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>dwi_file</th>\n",
       "      <th>bvec_file</th>\n",
       "      <th>bval_file</th>\n",
       "      <th>custom_tract</th>\n",
       "      <th>reg_subject</th>\n",
       "      <th>ses</th>\n",
       "      <th>timing</th>\n",
       "      <th>sl_counts</th>\n",
       "      <th>results_dir</th>\n",
       "      <th>gtab</th>\n",
       "      <th>dwi_affine</th>\n",
       "      <th>dwi_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103818</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>{'Tractography': 0, 'Registration': 0, 'Segmen...</td>\n",
       "      <td>n_streamlines  n_clean_streamline...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>&lt;dipy.core.gradients.GradientTable object at 0...</td>\n",
       "      <td>[[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...</td>\n",
       "      <td>&lt;class 'nibabel.nifti1.Nifti1Image'&gt;\\ndata sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105923</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>{'Tractography': 0, 'Registration': 0, 'Segmen...</td>\n",
       "      <td>n_streamlines  n_clean_streamline...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>&lt;dipy.core.gradients.GradientTable object at 0...</td>\n",
       "      <td>[[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...</td>\n",
       "      <td>&lt;class 'nibabel.nifti1.Nifti1Image'&gt;\\ndata sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111312</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>{'Tractography': 0, 'Registration': 0, 'Segmen...</td>\n",
       "      <td>n_streamlines  n_clean_streamline...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>&lt;dipy.core.gradients.GradientTable object at 0...</td>\n",
       "      <td>[[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...</td>\n",
       "      <td>&lt;class 'nibabel.nifti1.Nifti1Image'&gt;\\ndata sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114823</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>{'Tractography': 0, 'Registration': 0, 'Segmen...</td>\n",
       "      <td>n_streamlines  n_clean_streamline...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>&lt;dipy.core.gradients.GradientTable object at 0...</td>\n",
       "      <td>[[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...</td>\n",
       "      <td>&lt;class 'nibabel.nifti1.Nifti1Image'&gt;\\ndata sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115320</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01</td>\n",
       "      <td>{'Tractography': 0, 'Registration': 0, 'Segmen...</td>\n",
       "      <td>n_streamlines  n_clean_streamline...</td>\n",
       "      <td>/Users/bloomdt/AFQ_data/hcp_retest/derivatives...</td>\n",
       "      <td>&lt;dipy.core.gradients.GradientTable object at 0...</td>\n",
       "      <td>[[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...</td>\n",
       "      <td>&lt;class 'nibabel.nifti1.Nifti1Image'&gt;\\ndata sha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject                                           dwi_file  \\\n",
       "0  103818  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "1  105923  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "2  111312  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "3  114823  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "4  115320  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "\n",
       "                                           bvec_file  \\\n",
       "0  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "1  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "2  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "3  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "4  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "\n",
       "                                           bval_file custom_tract reg_subject  \\\n",
       "0  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...         None        None   \n",
       "1  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...         None        None   \n",
       "2  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...         None        None   \n",
       "3  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...         None        None   \n",
       "4  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...         None        None   \n",
       "\n",
       "  ses                                             timing  \\\n",
       "0  01  {'Tractography': 0, 'Registration': 0, 'Segmen...   \n",
       "1  01  {'Tractography': 0, 'Registration': 0, 'Segmen...   \n",
       "2  01  {'Tractography': 0, 'Registration': 0, 'Segmen...   \n",
       "3  01  {'Tractography': 0, 'Registration': 0, 'Segmen...   \n",
       "4  01  {'Tractography': 0, 'Registration': 0, 'Segmen...   \n",
       "\n",
       "                                           sl_counts  \\\n",
       "0               n_streamlines  n_clean_streamline...   \n",
       "1               n_streamlines  n_clean_streamline...   \n",
       "2               n_streamlines  n_clean_streamline...   \n",
       "3               n_streamlines  n_clean_streamline...   \n",
       "4               n_streamlines  n_clean_streamline...   \n",
       "\n",
       "                                         results_dir  \\\n",
       "0  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "1  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "2  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "3  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "4  /Users/bloomdt/AFQ_data/hcp_retest/derivatives...   \n",
       "\n",
       "                                                gtab  \\\n",
       "0  <dipy.core.gradients.GradientTable object at 0...   \n",
       "1  <dipy.core.gradients.GradientTable object at 0...   \n",
       "2  <dipy.core.gradients.GradientTable object at 0...   \n",
       "3  <dipy.core.gradients.GradientTable object at 0...   \n",
       "4  <dipy.core.gradients.GradientTable object at 0...   \n",
       "\n",
       "                                          dwi_affine  \\\n",
       "0  [[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...   \n",
       "1  [[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...   \n",
       "2  [[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...   \n",
       "3  [[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...   \n",
       "4  [[-1.25, 0.0, 0.0, 90.0], [0.0, 1.25, 0.0, -12...   \n",
       "\n",
       "                                             dwi_img  \n",
       "0  <class 'nibabel.nifti1.Nifti1Image'>\\ndata sha...  \n",
       "1  <class 'nibabel.nifti1.Nifti1Image'>\\ndata sha...  \n",
       "2  <class 'nibabel.nifti1.Nifti1Image'>\\ndata sha...  \n",
       "3  <class 'nibabel.nifti1.Nifti1Image'>\\ndata sha...  \n",
       "4  <class 'nibabel.nifti1.Nifti1Image'>\\ndata sha...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if compare_test_retest:\n",
    "    print('HCP')\n",
    "    myafq_test = get_afq('HCP')\n",
    "    display(myafq_test.data_frame)\n",
    "    \n",
    "    print('HCP_retest')\n",
    "    myafq_retest = get_afq('HCP_retest')\n",
    "    display(myafq_retest.data_frame)\n",
    "else:\n",
    "    print(dataset_name)\n",
    "    myafq = get_afq(dataset_name)\n",
    "    display(myafq.data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if compare_test_retest:\n",
    "#     bundle_names = [*myafq_retest.bundle_dict]\n",
    "# else:\n",
    "#     bundle_names = [*myafq.bundle_dict]\n",
    "\n",
    "# bundle_names = ['SLF_L', 'SLF_R']\n",
    "# bundle_names = ['ARC_L', 'ARC_R', 'CST_L', 'CST_R', 'FP'] \n",
    "# bundle_names = ['SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'CST_L', 'CST_R', 'FP']\n",
    "bundle_names = ['SLF_R']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "feature_files = {}\n",
    "target_dirs = {}\n",
    "fa_values = {}\n",
    "mean_warped_fa_values = {}\n",
    "pairwise_warped_fa_values = {}\n",
    "md_values = {}\n",
    "mean_warped_md_values = {}\n",
    "pairwise_warped_md_values = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    feature_files[subject] = {}\n",
    "    target_dirs[subject] = {}\n",
    "    fa_values[subject] = {}\n",
    "    mean_warped_fa_values[subject] = {}\n",
    "    pairwise_warped_fa_values[subject] = {}\n",
    "    md_values[subject] = {}\n",
    "    mean_warped_md_values[subject] = {}\n",
    "    pairwise_warped_md_values[subject] = {}\n",
    "        \n",
    "    if compare_test_retest:\n",
    "        loc_test  = get_iloc(myafq_test, subject)\n",
    "        loc_retest = get_iloc(myafq_retest, subject)\n",
    "    else:\n",
    "        loc = get_iloc(myafq, subject)\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        feature_files[subject][bundle_name] = {}\n",
    "        target_dirs[subject][bundle_name] = {}\n",
    "        fa_values[subject][bundle_name] = {}\n",
    "        mean_warped_fa_values[subject][bundle_name] = {}\n",
    "        pairwise_warped_fa_values[subject][bundle_name] = {}\n",
    "        md_values[subject][bundle_name] = {}\n",
    "        mean_warped_md_values[subject][bundle_name] = {}\n",
    "        pairwise_warped_md_values[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions, [myafq_test, myafq_retest], [loc_test, loc_retest])\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name], [myafq], [loc])\n",
    "\n",
    "        for name, ses, myafq, loc in iterables:\n",
    "            feature_files[subject][bundle_name][ses] = []\n",
    "            \n",
    "            target_dir = get_dir_name(myafq, dataset_name, bundle_name, loc)\n",
    "            target_dirs[subject][bundle_name][ses] = target_dir\n",
    "#             print(name, subject, bundle_name, ses, target_dir)\n",
    "\n",
    "            # TODO consider using myafq.scalars\n",
    "            # TODO consider optionally specifying scalars of interest\n",
    "\n",
    "            fa_f_name = op.join(target_dir, 'streamline_profile_fa.npy');\n",
    "            if op.isfile(fa_f_name):\n",
    "                feature_files[subject][bundle_name][ses].append(fa_f_name)\n",
    "                fa_values[subject][bundle_name][ses] = np.load(fa_f_name)\n",
    "    #             print(name, subject, bundle_name, ses, fa_f_name)\n",
    "            \n",
    "            md_f_name = op.join(target_dir, 'streamline_profile_md.npy')\n",
    "            if op.isfile(md_f_name):\n",
    "                feature_files[subject][bundle_name][ses].append(md_f_name)\n",
    "                md_values[subject][bundle_name][ses] = np.load(md_f_name)\n",
    "    #             print(name, subject, bundle_name, ses, md_f_name)\n",
    "\n",
    "            # reference warped: mean\n",
    "            fa_f_name = op.join(target_dir, 'streamline_profile_ref_warped_fa.npy')\n",
    "            if op.isfile(fa_f_name):\n",
    "                feature_files[subject][bundle_name][ses].append(fa_f_name)\n",
    "                mean_warped_fa_values[subject][bundle_name][ses] = np.load(fa_f_name)\n",
    "    #             print(name, subject, bundle_name, ses, fa_f_name)\n",
    "\n",
    "            md_f_name = op.join(target_dir, 'streamline_profile_ref_warped_md.npy')\n",
    "            if op.isfile(md_f_name):\n",
    "                feature_files[subject][bundle_name][ses].append(md_f_name)\n",
    "                mean_warped_md_values[subject][bundle_name][ses] = np.load(md_f_name)\n",
    "    #             print(name, subject, bundle_name, ses, md_f_name)\n",
    "\n",
    "            # pairwise warped\n",
    "            fa_f_name = op.join(target_dir, 'streamline_profile_pairwise_warped_fa.npy')\n",
    "            if op.isfile(fa_f_name):\n",
    "                feature_files[subject][bundle_name][ses].append(fa_f_name)\n",
    "                pairwise_warped_fa_values[subject][bundle_name][ses] = np.load(fa_f_name)\n",
    "    #             print(name, subject, bundle_name, ses, fa_f_name)\n",
    "\n",
    "            md_f_name = op.join(target_dir, 'streamline_profile_pairwise_warped_md.npy')\n",
    "            if op.isfile(md_f_name):\n",
    "                feature_files[subject][bundle_name][ses].append(md_f_name)\n",
    "                pairwise_warped_md_values[subject][bundle_name][ses] = np.load(md_f_name)\n",
    "    #             print(name, subject, bundle_name, ses, md_f_name)\n",
    "\n",
    "            # validation information\n",
    "            validate = False\n",
    "            \n",
    "            if validate:\n",
    "                print(name, subject, bundle_name, ses)\n",
    "                streamline_tract_profile_test(fa_values[subject][bundle_name][ses])\n",
    "                streamline_tract_profile_test(mean_warped_fa_values[subject][bundle_name][ses])\n",
    "                streamline_tract_profile_test(md_values[subject][bundle_name][ses])\n",
    "                streamline_tract_profile_test(mean_warped_md_values[subject][bundle_name][ses])\n",
    "\n",
    "features_df = pd.DataFrame.from_dict(\n",
    "    {(i,j,k): feature_files[i][j][k] for i in feature_files.keys() for j in feature_files[i].keys() for k in feature_files[i][j].keys()}, \n",
    "    orient='index', \n",
    "    columns=['fa', 'md', 'mean_fa', 'mean_md', 'pairwise_fa', 'pairwise_md']\n",
    ")\n",
    "\n",
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(features_df)\n",
    "    \n",
    "os.makedirs(op.join('subbundles', dataset_name), exist_ok=True)\n",
    "f_name = op.join('subbundles', dataset_name, f'feature_files.csv')\n",
    "print(f_name)\n",
    "features_df.to_csv(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dirs = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    target_dirs[subject] = {}\n",
    "    \n",
    "    if compare_test_retest:\n",
    "        loc_test  = get_iloc(myafq_test, subject)\n",
    "        loc_retest = get_iloc(myafq_retest, subject)\n",
    "    else:\n",
    "        loc = get_iloc(myafq, subject)\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        target_dirs[subject][bundle_name] = {}\n",
    "        \n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions, [myafq_test, myafq_retest], [loc_test, loc_retest])\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name], [myafq], [loc])\n",
    "\n",
    "        for name, ses, myafq, loc in iterables:\n",
    "            target_dir = get_dir_name(myafq, dataset_name, bundle_name, loc)\n",
    "            target_dirs[subject][bundle_name][ses] = target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: There are multiple python implementations to compute correlation</span>\n",
    "\n",
    "- Haven't [compared or contrasted benefits](https://realpython.com/numpy-scipy-pandas-correlation-python/) of `NumPy`, `SciPy`, or `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Candidiate [Correlation Coefficients](https://en.wikipedia.org/wiki/Correlation_coefficient#Types):\n",
    "\n",
    "- Pearson's r\n",
    "\n",
    "- [Rank correlation](https://en.wikipedia.org/wiki/Rank_correlation) coefficients\n",
    "\n",
    "  - Spearman's $\\rho$\n",
    "\n",
    "  - Kendall's $\\tau$\n",
    "  \n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FA (Fractional Anisotropy)\n",
    "\n",
    "One of the scalars calculated from DWI.\n",
    "\n",
    "More focus has been placed on FA since Schurr's paper indicates FA can be used to isolate SLF subbundles."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fa_dfs = {}\n",
    "fa_corrs = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    fa_dfs[subject] = {}\n",
    "    fa_corrs[subject] = {}\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        fa_dfs[subject][bundle_name] = {}\n",
    "        fa_corrs[subject][bundle_name] = {}\n",
    "        \n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in fa_values[subject][bundle_name]:\n",
    "                print(dataset_name, subject, bundle_name, 'fa', 'no values')\n",
    "                continue\n",
    "            \n",
    "            fa_df = pd.DataFrame(fa_values[subject][bundle_name][ses].T)\n",
    "            fa_dfs[subject][bundle_name][ses] = fa_df\n",
    "            fa_corr = fa_df.corr()\n",
    "            fa_corrs[subject][bundle_name][ses] = fa_corr\n",
    "\n",
    "            if not compare_test_retest:\n",
    "                target_dir = target_dirs[subject][bundle_name][ses]\n",
    "                f_name = op.join(target_dir,'adjacency_fa.npy')\n",
    "                np.save(f_name, fa_corr.to_numpy())\n",
    "                print(dataset_name, subject, bundle_name, 'fa', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_corrs = {}\n",
    "for subject in subjects:\n",
    "    fa_corrs[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        fa_corrs[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_fa.npy')\n",
    "            if op.isfile(f_name):\n",
    "                fa_corrs[subject][bundle_name][ses] = np.load(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Difference in $\\mu$ FA (mean Fractional Anisotropy)\n",
    "\n",
    "Origially requested by Jason, but supplanted in favor of coefficient of determiniation since it also takes into account variance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not compare_test_retest:\n",
    "    for subject in subjects:\n",
    "        for bundle_name in bundle_names:\n",
    "            fa_df = fa_dfs[subject][bundle_name]\n",
    "\n",
    "            fa_mu = pd.DataFrame([[np.absolute(a-b) for a in fa_df.mean()] for b in fa_df.mean()])\n",
    "\n",
    "            f_name = op.join(target_dir,'adjacency_fa_mu.npy')\n",
    "            np.save(f_name, fa_mu.to_numpy())\n",
    "            print(dataset_name, subject, bundle_name, 'fa_mu', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient of Determination: $R^2$\n",
    "\n",
    "In Schurr 2019 paper, main differentiating factors between SLFII and the other SLF sections are offsets in the mean FA between streamlines. \n",
    "\n",
    "Consider use a distance function that takes into account the offset, and not only co-variation, like:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### examplar coefficent of determination"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def coeff_of_determination(data, model, axis=-1):\n",
    "    \"\"\"\n",
    "     http://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "              _                                            _\n",
    "             |    sum of the squared residuals              |\n",
    "    R^2 =    |1 - ---------------------------------------   | * 100\n",
    "             |_    sum of the squared mean-subtracted data _|\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.empty((data.shape[0], model.shape[0]))\n",
    "    demeaned_data = data - np.mean(data, axis=axis)[...,np.newaxis]\n",
    "    ss_tot = np.sum(demeaned_data **2, axis=axis)\n",
    "    # Don't divide by 0:\n",
    "    if np.all(ss_tot==0.0):\n",
    "        X[:, :] = np.nan\n",
    "        return X\n",
    "\n",
    "    for ii in range(X.shape[0]):\n",
    "        for jj in range(X.shape[1]):\n",
    "            # There's no point in doing any of this: \n",
    "            if np.all(data[ii]==0.0) and np.all(model[ii]==0.0):\n",
    "                X[ii, jj] = np.nan\n",
    "            else:\n",
    "                residuals = data[ii] - model[jj]\n",
    "                ss_err = np.sum(residuals ** 2, axis=axis)\n",
    "                X[ii, jj] = 1 - (ss_err/ss_tot[ii])\n",
    "\n",
    "    \n",
    "    return X\n",
    "\n",
    "fa_r2_ = coeff_of_determination(fa_values, fa_values)\n",
    "adjacency_test(fa_r2)\n",
    "adjacency_test(fa_r2_)\n",
    "fa_r2-fa_r2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FA $R^2$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fa_r2s = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    fa_r2s[subject] = {}\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        fa_r2s[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_fa_r2.npy')\n",
    "            \n",
    "            if compare_test_retest:    \n",
    "                fa_r2s[subject][bundle_name][ses] = np.load(f_name)\n",
    "                print(name, subject, bundle_name, 'fa_r2', 'loading', f_name)\n",
    "            else:\n",
    "                if not ses in fa_values[subject][bundle_name]:\n",
    "                    print(dataset_name, subject, bundle_name, 'fa_r2', 'no values')\n",
    "                    continue\n",
    "                    \n",
    "                values = fa_values[subject][bundle_name][ses]\n",
    "                \n",
    "                tic = time.perf_counter()\n",
    "                fa_r2 = pd.DataFrame([[r2_score(a,b) for a in values] for b in values])\n",
    "                # Note: to make symmetric calculate inverse then average\n",
    "                fa_r2 += fa_r2.T\n",
    "                fa_r2 = fa_r2/2\n",
    "                fa_r2s[subject][bundle_name][ses] = fa_r2\n",
    "                toc = time.perf_counter()\n",
    "                print(dataset_name, subject, bundle_name, f'{toc - tic:0.4f} seconds')\n",
    "\n",
    "                np.save(f_name, fa_r2.to_numpy())\n",
    "                print(dataset_name, subject, bundle_name, 'fa_r2', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_r2s = {}\n",
    "for subject in subjects:\n",
    "    fa_r2s[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        fa_r2s[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_fa_r2.npy')\n",
    "            if op.isfile(f_name):\n",
    "                fa_r2s[subject][bundle_name][ses] = np.load(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   $\\mu$-Reference  Warped FA $R^2$\n",
    "\n",
    "The streamline profiles have been warped based on a reference streamline. Here the reference is a pseudo-streamline with the mean FA value across the bundle at each point."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean_warped_fa_r2s = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    mean_warped_fa_r2s[subject] = {}\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        mean_warped_fa_r2s[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_ref_warped_fa_r2.npy')\n",
    "            \n",
    "            if compare_test_retest:\n",
    "                mean_warped_fa_r2s[subject][bundle_name][ses] = np.load(f_name)\n",
    "                print(name, subject, bundle_name, 'warped_fa_r2', 'loading', f_name)\n",
    "            else:\n",
    "                if not ses in fa_values[subject][bundle_name]:\n",
    "                    print(dataset_name, subject, bundle_name, 'warped_fa_r2', 'no values')\n",
    "                    continue\n",
    "                \n",
    "                values = mean_warped_fa_values[subject][bundle_name][ses]\n",
    "\n",
    "                tic = time.perf_counter()\n",
    "                fa_r2 = pd.DataFrame([[r2_score(a,b) for a in values] for b in values])\n",
    "                # Note: to make symmetric calculate inverse then average\n",
    "                fa_r2 += fa_r2.T\n",
    "                fa_r2 = fa_r2/2\n",
    "                mean_warped_fa_r2s[subject][bundle_name][ses] = fa_r2\n",
    "                toc = time.perf_counter()\n",
    "                print(dataset_name, subject, bundle_name, f'{toc - tic:0.4f} seconds')\n",
    "\n",
    "                np.save(f_name, fa_r2.to_numpy())\n",
    "                print(dataset_name, subject, bundle_name, 'warped_fa_r2', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_warped_fa_r2s = {}\n",
    "for subject in subjects:\n",
    "    mean_warped_fa_r2s[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        mean_warped_fa_r2s[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_ref_warped_fa_r2.npy')\n",
    "            if op.isfile(f_name):\n",
    "                mean_warped_fa_r2s[subject][bundle_name][ses] = np.load(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Pairwise Warped FA $R^2$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pairwise_warped_fa_r2s = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    pairwise_warped_fa_r2s[subject] = {}\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        pairwise_warped_fa_r2s[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_pairwise_warped_fa_r2.npy')\n",
    "            \n",
    "            if compare_test_retest:\n",
    "                pairwise_warped_fa_r2s[subject][bundle_name][ses] = np.load(f_name)\n",
    "                print(name, subject, bundle_name, 'pairwise_warped_fa_r2', 'loading', f_name)\n",
    "            else:\n",
    "                if not ses in fa_values[subject][bundle_name]:\n",
    "                    print(dataset_name, subject, bundle_name, 'pairwise_warped_fa_r2', 'no values')\n",
    "                    continue\n",
    "                \n",
    "                # To calculate a NxN for the R^2, compare each warped streamline profile \n",
    "                # to the unwarped target streamline\n",
    "                unwarped_fa_values = fa_values[subject][bundle_name][ses]\n",
    "                warped_fa_values = pairwise_warped_fa_values[subject][bundle_name][ses]\n",
    "\n",
    "                tic = time.perf_counter()\n",
    "                fa_r2 = pd.DataFrame(\n",
    "                    [\n",
    "                        [\n",
    "                            r2_score(warped_fa_values[i][j], unwarped_fa_values[i]) \n",
    "                             for j in np.ndindex(warped_fa_values.shape[1])\n",
    "                        ]\n",
    "                        for i in np.ndindex(warped_fa_values.shape[0])\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Note: to make symmetric calculate inverse then average\n",
    "                fa_r2 += fa_r2.T\n",
    "                fa_r2 = fa_r2/2\n",
    "                toc = time.perf_counter()\n",
    "                print(dataset_name, subject, bundle_name, f'{toc - tic:0.4f} seconds')\n",
    "                print(fa_r2.shape)\n",
    "\n",
    "                pairwise_warped_fa_r2s[subject][bundle_name][ses] = fa_r2\n",
    "\n",
    "                np.save(f_name, fa_r2.to_numpy())\n",
    "                print(dataset_name, subject, bundle_name, 'pairwise_warped_fa_r2', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_warped_fa_r2s = {}\n",
    "for subject in subjects:\n",
    "    pairwise_warped_fa_r2s[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        pairwise_warped_fa_r2s[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_pairwise_warped_fa_r2.npy')\n",
    "            if op.isfile(f_name):\n",
    "                pairwise_warped_fa_r2s[subject][bundle_name][ses] = np.load(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MD (Mean Diffusivity)\n",
    "\n",
    "One of the scalars calculated from DWI"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "md_dfs = {}\n",
    "md_corrs = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    md_dfs[subject] = {}\n",
    "    md_corrs[subject] = {}\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        md_dfs[subject][bundle_name] = {}\n",
    "        md_corrs[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in md_values[subject][bundle_name]:\n",
    "                print(dataset_name, subject, bundle_name, 'md', 'no values')\n",
    "                continue\n",
    "                \n",
    "            md_df = pd.DataFrame(md_values[subject][bundle_name][ses].T)\n",
    "            md_dfs[subject][bundle_name][ses] = md_df\n",
    "\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_md.npy')\n",
    "            \n",
    "            if compare_test_retest:\n",
    "                md_corrs[subject][bundle_name][ses] = np.load(f_name)\n",
    "                print(name, subject, bundle_name, 'md', 'loading', f_name)\n",
    "            else:\n",
    "                md_corr = md_df.corr()\n",
    "                md_corrs[subject][bundle_name][ses] = md_corr\n",
    "            \n",
    "                np.save(f_name, md_corr.to_numpy())\n",
    "                print(dataset_name, subject, bundle_name, 'md', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_corrs = {}\n",
    "for subject in subjects:\n",
    "    md_corrs[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        md_corrs[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_md.npy')\n",
    "            if op.isfile(f_name):\n",
    "                md_corrs[subject][bundle_name][ses] = np.load(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics\n",
    "\n",
    "Given: \n",
    "\n",
    "- NxN streamlines \n",
    "\n",
    "Assuming:\n",
    "\n",
    "- same subject\n",
    "\n",
    "- same streamlines\n",
    "\n",
    "Anything varies streamlines would violate assumptions:\n",
    "\n",
    "- comparing subjects\n",
    "\n",
    "- using different tractometry, segmentation, or metrics\n",
    "\n",
    "Want:\n",
    "\n",
    "- a distance metric that is similar to correlation\n",
    "\n",
    "  - bounded between 0 and 1\n",
    "  \n",
    "  - 0 signifies streamlines are infinitely far apart\n",
    "  \n",
    "  - 1 signifies same streamline\n",
    "\n",
    "Consider: \n",
    "\n",
    "- MDF between every pair\n",
    "\n",
    "- Threshold distance $\\theta$\n",
    "\n",
    "  - If distance is greater than threshold then streamlines are considered infinitely far apart (not part of the same subbundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDF (Minimum average Direct-Flip)\n",
    "\n",
    "https://dipy.org/documentation/1.2.0./reference/dipy.segment/#bundles-distances-mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCP_retest 105923 SLF_R 1.3212 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symmetric:  True\n",
      "finite:  True\n",
      "min:  0.0\n",
      "max:  1.0\n",
      "HCP_retest 105923 SLF_R is_mdf saving subbundles/HCP_retest/SLF_R/105923/01/adjacency_is_mdf.npy\n"
     ]
    }
   ],
   "source": [
    "mdfs = {}\n",
    "is_mdfs = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    mdfs[subject] = {}\n",
    "    is_mdfs[subject] = {}\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        mdfs[subject][bundle_name] = {}\n",
    "        is_mdfs[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_mdf.npy')\n",
    "\n",
    "            if compare_test_retest:\n",
    "                if op.isfile(f_name):\n",
    "                    mdfs[subject][bundle_name][ses] = np.load(f_name)\n",
    "                    print(name, subject, bundle_name, 'mdf', 'loading', f_name)\n",
    "                \n",
    "                f_name = op.join(target_dir,'adjacency_is_mdf.npy')\n",
    "\n",
    "                if op.isfile(f_name):\n",
    "                    is_mdfs[subject][bundle_name][ses] = np.load(f_name)\n",
    "                    print(name, subject, bundle_name, 'is_mdf', 'loading', f_name)\n",
    "            else:\n",
    "                loc = get_iloc(myafq, subject)            \n",
    "                tg_fname = get_tractogram_filename(myafq, bundle_name, loc)\n",
    "                tractogram = load_tractogram(tg_fname, 'same')\n",
    "                streamlines = tractogram.streamlines\n",
    "                affine = tractogram.affine\n",
    "                \n",
    "                if len(streamlines) == 0:\n",
    "                    print(dataset_name, subject, bundle_name, 'mdf', 'no streamlines')\n",
    "                    continue\n",
    "\n",
    "                fgarray = np.array(dts.set_number_of_points(streamlines, 100))\n",
    "                            \n",
    "                tic = time.perf_counter()\n",
    "                mdf_np = dts.bundles_distances_mdf(fgarray, fgarray)\n",
    "                # enforce symmetry (address numerical errors from mdf)\n",
    "                mdf_np = (mdf_np + mdf_np.T) / 2\n",
    "                mdf = pd.DataFrame(mdf_np)\n",
    "                mdfs[subject][bundle_name][ses] = mdf\n",
    "\n",
    "                # np.save(op.join(target_dir,'adjacency_mdf.npy'), mdf.to_numpy())\n",
    "                \n",
    "                ########## Investigating MDF symmetry ##########\n",
    "                \n",
    "#                 adjacency_test(mdf_np)\n",
    "#                 print('mdf nonzero diff:', np.count_nonzero(mdf_np-mdf_np.T))\n",
    "#                 print('mdf nonzero diff idx:', np.nonzero(mdf_np-mdf_np.T))\n",
    "#                 print('mdf sum abs diff:', np.sum(abs(mdf_np-mdf_np.T)))\n",
    "#                 print('mdf max abs diff:', np.max(abs(mdf_np-mdf_np.T)))\n",
    "#                 i,j = np.unravel_index(np.argmax(abs(mdf_np-mdf_np.T), axis=None), abs(mdf_np-mdf_np.T).shape)\n",
    "#                 print('mdf distances:\\n', dts.bundles_distances_mdf([fgarray[i], fgarray[j]],[fgarray[i], fgarray[j]]))\n",
    "#                 ax = plt.axes(projection='3d')\n",
    "#                 ax.plot3D(fgarray[i,:,0],fgarray[i,:,1],fgarray[i,:,2], label=i)\n",
    "#                 ax.plot3D(fgarray[j,:,0],fgarray[j,:,1],fgarray[j,:,2], label=j)\n",
    "#                 plt.title('max asymmetric MDF')\n",
    "#                 ax.legend()\n",
    "#                 plt.show()\n",
    "                \n",
    "#                 np.save('max_asymmetric_mdf.npy', [fgarray[i], fgarray[j]])\n",
    "\n",
    "#                 for i,j in np.transpose(np.nonzero(mdf_np-mdf_np.T)):\n",
    "#                     print('mdf distances:\\n', dts.bundles_distances_mdf([fgarray[i], fgarray[j]],[fgarray[i], fgarray[j]]))\n",
    "#                     ax = plt.axes(projection='3d')\n",
    "#                     ax.plot3D(fgarray[i,:,0],fgarray[i,:,1],fgarray[i,:,2], label=i)\n",
    "#                     ax.plot3D(fgarray[j,:,0],fgarray[j,:,1],fgarray[j,:,2], label=j)\n",
    "#                     plt.title('asymmetric MDF')\n",
    "#                     ax.legend()\n",
    "#                     plt.show()\n",
    "#                     break\n",
    "                    \n",
    "                ########## Investigating MDF symmetry ##########\n",
    "\n",
    "\n",
    "                # inverse scaled mdf (is_mdf)\n",
    "\n",
    "                mdf_max = mdf.to_numpy().max()\n",
    "                is_mdf = (mdf_max - mdf)\n",
    "                is_mdf_max = is_mdf.to_numpy().max()\n",
    "                is_mdf = is_mdf / is_mdf_max\n",
    "                is_mdfs[subject][bundle_name][ses] = is_mdf\n",
    "                toc = time.perf_counter()\n",
    "                print(dataset_name, subject, bundle_name, f'{toc - tic:0.4f} seconds')\n",
    "                \n",
    "                adjacency_test(is_mdf)\n",
    "                \n",
    "                f_name = op.join(target_dir,'adjacency_is_mdf.npy')\n",
    "                np.save(f_name, is_mdf.to_numpy())\n",
    "                print(dataset_name, subject, bundle_name, 'is_mdf', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Asymmetry MDF for Defect\n",
    "\n",
    "**TODO Open issue on dipy**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "is_mdfs = {}\n",
    "for subject in subjects:\n",
    "    is_mdfs[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        is_mdfs[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            f_name = op.join(target_dir,'adjacency_is_mdf.npy')\n",
    "            if op.isfile(f_name):\n",
    "                is_mdfs[subject][bundle_name][ses] = np.load(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Adjacencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ FA + $\\beta$ MD "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "fa_md_wts = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    fa_md_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        fa_md_wts[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:          \n",
    "            fa_md_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "\n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_fa_{int(beta*10)}_md.npy')\n",
    "\n",
    "                if compare_test_retest:\n",
    "                    fa_md_wts[subject][bundle_name][ses].append(np.load(f_name))\n",
    "                    print(name, subject, bundle_name, 'fa_md_wts', 'loading', f_name)\n",
    "                else:\n",
    "                    weighted = alpha * fa_corrs[subject][bundle_name][ses] + beta * md_corrs[subject][bundle_name][ses]\n",
    "                    fa_md_wts[subject][bundle_name][ses].append(weighted)\n",
    "                    np.save(f_name,  weighted.to_numpy())\n",
    "                    print(dataset_name, subject, bundle_name, 'fa_md_wts', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_md_wts = {}\n",
    "for subject in subjects:\n",
    "    fa_md_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        fa_md_wts[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            fa_md_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_fa_{int(beta*10)}_md.npy')\n",
    "                if op.isfile(f_name):\n",
    "                    fa_md_wts[subject][bundle_name][ses].append(np.load(f_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ FA $R^2$ + $\\beta$ MDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "far2_mdf_wts = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    far2_mdf_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        far2_mdf_wts[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            far2_mdf_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            if not ses in fa_r2s[subject][bundle_name] or not ses in is_mdfs[subject][bundle_name]:\n",
    "                print(dataset_name, subject, bundle_name, 'fa_r2_is_mdf_wts', 'no values')\n",
    "                continue\n",
    "\n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_fa_r2_{int(beta*10)}_is_mdf.npy')\n",
    "                \n",
    "                if compare_test_retest:\n",
    "                    far2_mdf_wts[subject][bundle_name][ses].append(np.load(f_name))\n",
    "                    print(name, subject, bundle_name, 'fa_r2_is_mdf_wts', 'loading', f_name)\n",
    "                else:\n",
    "                    wt_far2_mdf = alpha * fa_r2s[subject][bundle_name][ses] + beta * is_mdfs[subject][bundle_name][ses]\n",
    "                    far2_mdf_wts[subject][bundle_name][ses].append(wt_far2_mdf)\n",
    "                    \n",
    "                    np.save(f_name, wt_far2_mdf.to_numpy())\n",
    "                    print(dataset_name, subject, bundle_name, 'fa_r2_is_mdf_wts', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "far2_mdf_wts = {}\n",
    "for subject in subjects:\n",
    "    far2_mdf_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        far2_mdf_wts[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            far2_mdf_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_fa_r2_{int(beta*10)}_is_mdf.npy')\n",
    "                if op.isfile(f_name):\n",
    "                    far2_mdf_wts[subject][bundle_name][ses].append(np.load(f_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$  $\\mu$-Reference Warped FA $R^2$ + $\\beta$ MDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "mean_warped_far2_mdf_wts = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    mean_warped_far2_mdf_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        mean_warped_far2_mdf_wts[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            mean_warped_far2_mdf_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            if not ses in mean_warped_fa_r2s[subject][bundle_name] or not ses in is_mdfs[subject][bundle_name]:\n",
    "                print(dataset_name, subject, bundle_name, 'ref_warped_fa_r2_is_mdf_wts', 'no values')\n",
    "                continue\n",
    "\n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_ref_warped_fa_r2_{int(beta*10)}_is_mdf.npy')\n",
    "\n",
    "                if compare_test_retest:\n",
    "                    mean_warped_far2_mdf_wts[subject][bundle_name][ses].append(np.load(f_name))\n",
    "                    print(name, subject, bundle_name, 'ref_warped_fa_r2_is_mdf_wts', 'loading', f_name)\n",
    "                else:\n",
    "                    wt_warpedfar2_mdf = alpha * mean_warped_fa_r2s[subject][bundle_name][ses] + beta * is_mdfs[subject][bundle_name][ses]\n",
    "                    mean_warped_far2_mdf_wts[subject][bundle_name][ses].append(wt_warpedfar2_mdf)\n",
    "\n",
    "                    np.save(f_name, wt_warpedfar2_mdf.to_numpy())\n",
    "                    print(dataset_name, subject, bundle_name, 'ref_warped_fa_r2_is_mdf_wts', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "mean_warped_far2_mdf_wts = {}\n",
    "for subject in subjects:\n",
    "    mean_warped_far2_mdf_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        mean_warped_far2_mdf_wts[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            mean_warped_far2_mdf_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_ref_warped_fa_r2_{int(beta*10)}_is_mdf.npy')\n",
    "                if op.isfile(f_name):\n",
    "                    mean_warped_far2_mdf_wts[subject][bundle_name][ses].append(np.load(f_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ Pairwise Warped FA $R^2$ + $\\beta$ MDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "pairwise_warped_far2_mdf_wts = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    pairwise_warped_far2_mdf_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        pairwise_warped_far2_mdf_wts[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            pairwise_warped_far2_mdf_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            if not ses in pairwise_warped_fa_r2s[subject][bundle_name] or not ses in is_mdfs[subject][bundle_name]:\n",
    "                print(dataset_name, subject, bundle_name, 'pairwisewarpedfar2_mdf_wts', 'no values')\n",
    "                continue\n",
    "\n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_pairwise_warped_fa_r2_{int(beta*10)}_is_mdf.npy')\n",
    "                \n",
    "                if compare_test_retest:\n",
    "                    pairwise_warped_far2_mdf_wts[subject][bundle_name][ses].append(np.load(f_name))\n",
    "                    print(name, subject, bundle_name, 'pairwisewarpedfar2_mdf_wts', 'loading', f_name)\n",
    "                else:\n",
    "                    wt_warpedfar2_mdf = alpha * pairwise_warped_fa_r2s[subject][bundle_name][ses] + beta * is_mdfs[subject][bundle_name][ses]\n",
    "                    pairwise_warped_far2_mdf_wts[subject][bundle_name][ses].append(wt_warpedfar2_mdf)\n",
    "\n",
    "                    np.save(f_name, wt_warpedfar2_mdf.to_numpy())\n",
    "                    print(dataset_name, subject, bundle_name, 'pairwisewarpedfar2_mdf_wts', 'saving', f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0,10,11)/10\n",
    "betas = np.linspace(10,0,11)/10\n",
    "\n",
    "pairwise_warped_far2_mdf_wts = {}\n",
    "for subject in subjects:\n",
    "    pairwise_warped_far2_mdf_wts[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        pairwise_warped_far2_mdf_wts[subject][bundle_name] = {}\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            pairwise_warped_far2_mdf_wts[subject][bundle_name][ses] = []\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            for alpha, beta in zip(alphas, betas):\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_pairwise_warped_fa_r2_{int(beta*10)}_is_mdf.npy')\n",
    "                if op.isfile(f_name):\n",
    "                    pairwise_warped_far2_mdf_wts[subject][bundle_name][ses].append(np.load(f_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Matricies\n",
    "\n",
    "Once get streamline correlation matricies:\n",
    "\n",
    "- Begin with \"eye-ball comparison\" between FA and MD matrices\n",
    "\n",
    "- Then consider difference of adjacency matrices\n",
    "\n",
    "\n",
    "- <span style=\"color:red\">**Question: What is the 'correct' way compare these matricies?**</span>\n",
    "\n",
    "  - Check how much information is shared\n",
    "  \n",
    "    mutual information: $\\mathbb{I}(X,Y)$\n",
    "    \n",
    "  - Or alternatively, information only present in only one of the matricies\n",
    "  \n",
    "    i.e. sum of conditional information: $\\mathbb{H}(Y|X)+\\mathbb{H}(X|Y)$\n",
    "    \n",
    "      equivallently, cross entropy minus mutial information: $\\mathbb{H}(X,Y)-\\mathbb{I}(X,Y)$\n",
    "\n",
    "<span style=\"color:red\">**NOTE: we are assuming that streamlines are same (from same individual and same same tractography)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested finite and symmetric matricies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  \"Eye-ball\" Absolute Difference\n",
    "\n",
    "This was a first pass, now not as informative"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} FA - MD adjacency')\n",
    "            del_corr = np.abs(fa_corrs[subject][bundle_name][ses]-md_corrs[subject][bundle_name][ses])\n",
    "            plt.imshow(del_corr, cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_fa_md_diff.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FA\n",
    "\n",
    "Baseline FA hasn't been that useful, but have been using as starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in fa_corrs[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} FA adjacency')\n",
    "            plt.imshow(fa_corrs[subject][bundle_name][ses], cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_fa.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FA $R^2$\n",
    "\n",
    "Baseline FA R2. This doesn't account for offsets in streamlines and other streamline alignment issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in fa_r2s[subject][bundle_name]:\n",
    "                continue\n",
    "\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} FA $R^2$ adjacency')\n",
    "            plt.imshow(fa_r2s[subject][bundle_name][ses], cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_fa_r2.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            adjacency_test(fa_r2s[subject][bundle_name][ses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  $\\mu$-Reference  Warped FA $R^2$\n",
    "\n",
    "One of two warpings exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in mean_warped_fa_r2s[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} $\\mu$ Warped FA $R^2$ adjacency')\n",
    "            plt.imshow(mean_warped_fa_r2s[subject][bundle_name][ses], cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_ref_warped_fa_r2.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            adjacency_test(mean_warped_fa_r2s[subject][bundle_name][ses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pairwise Warped FA $R^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in pairwise_warped_fa_r2s[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} Pairwise Warped FA $R^2$ adjacency')\n",
    "            plt.imshow(pairwise_warped_fa_r2s[subject][bundle_name][ses], cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_pairwise_warped_fa_r2.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            adjacency_test(pairwise_warped_fa_r2s[subject][bundle_name][ses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MD\n",
    "\n",
    "Currently not exploiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in md_corrs[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} MD adjacency')\n",
    "            plt.imshow(md_corrs[subject][bundle_name][ses], cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_md.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            adjacency_test(md_corrs[subject][bundle_name][ses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDF\n",
    "\n",
    "Adds spatial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            if not ses in is_mdfs[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            plt.figure()\n",
    "            plt.title(f'{name} {subject} {bundle_name} {ses} MDF adjacency')\n",
    "            plt.imshow(is_mdfs[subject][bundle_name][ses], cmap='hot', interpolation='nearest')\n",
    "            plt.xlabel('streamline index')\n",
    "            plt.ylabel('streamline index')\n",
    "            plt.colorbar()\n",
    "            f_name = op.join(target_dir, f'adjacency_is_mdf.png')\n",
    "            print(f_name)\n",
    "            plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            adjacency_test(is_mdfs[subject][bundle_name][ses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$ FA $R^2$ + $\\beta$ MDF\n",
    "\n",
    "Initial weighted combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            if not ses in far2_mdf_wts[subject][bundle_name]:\n",
    "                continue\n",
    "\n",
    "            for far2_mdf_wt, alpha, beta in zip(far2_mdf_wts[subject][bundle_name][ses], alphas, betas):\n",
    "                plt.figure()\n",
    "                plt.title(f'{name} {subject} {bundle_name} {ses} FA $R^2$ + MDF adjacency')\n",
    "                plt.imshow(far2_mdf_wt, cmap='hot', interpolation='nearest')\n",
    "                plt.xlabel('streamline index')\n",
    "                plt.ylabel('streamline index')\n",
    "                plt.colorbar()\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_fa_r2_{int(beta*10)}_is_mdf.png')\n",
    "                print(f_name)\n",
    "                plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "                plt.show()\n",
    "\n",
    "                adjacency_test(far2_mdf_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$  $\\mu$-Reference Warped FA $R^2$ + $\\beta$ MDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            if not ses in mean_warped_far2_mdf_wts[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            for mean_warped_far2_mdf_wt, alpha, beta in zip(mean_warped_far2_mdf_wts[subject][bundle_name][ses], alphas, betas):\n",
    "                plt.figure()\n",
    "                plt.title(f'{name} {subject} {bundle_name} {ses} $\\mu$ Warped FA $R^2$ + MDF adjacency')\n",
    "                plt.imshow(mean_warped_far2_mdf_wt, cmap='hot', interpolation='nearest')\n",
    "                plt.xlabel('streamline index')\n",
    "                plt.ylabel('streamline index')\n",
    "                plt.colorbar()\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_ref_warped_fa_r2_{int(beta*10)}_is_mdf.png')\n",
    "                print(f_name)\n",
    "                plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "                plt.show()\n",
    "\n",
    "                adjacency_test(mean_warped_far2_mdf_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\alpha$  Pairwise Warped FA $R^2$ + $\\beta$ MDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([''], [dataset_name])\n",
    "            \n",
    "        for name, ses in iterables:\n",
    "            target_dir = target_dirs[subject][bundle_name][ses]\n",
    "            \n",
    "            if not ses in pairwise_warped_far2_mdf_wts[subject][bundle_name]:\n",
    "                continue\n",
    "                \n",
    "            for pairwise_warped_far2_mdf_wt, alpha, beta in zip(pairwise_warped_far2_mdf_wts[subject][bundle_name][ses], alphas, betas):\n",
    "                plt.figure()\n",
    "                plt.title(f'{name} {subject} {bundle_name} {ses} Pairwise Warped FA $R^2$ + MDF adjacency')\n",
    "                plt.imshow(pairwise_warped_far2_mdf_wt, cmap='hot', interpolation='nearest')\n",
    "                plt.xlabel('streamline index')\n",
    "                plt.ylabel('streamline index')\n",
    "                plt.colorbar()\n",
    "                f_name = op.join(target_dir, f'adjacency_wt_{int(alpha*10)}_pairwise_warped_fa_r2_{int(beta*10)}_is_mdf.png')\n",
    "                print(f_name)\n",
    "                plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "                plt.show()\n",
    "\n",
    "                adjacency_test(pairwise_warped_far2_mdf_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
