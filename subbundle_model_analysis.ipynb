{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subbundle Model Analysis\n",
    "\n",
    "At this point, have built clustering models for each subject across sessions for each bundle. The results for each expirement -- consisting of feature selection and embedding, choice of clustring algorithm and corresponding model hyperparameters -- is saved to:\n",
    "\n",
    "> s3://hcp-subbundle/`expirement_name`/`session_name`/`bundle_name`/`subject`/`n_clusters`/\n",
    "\n",
    "Each model folder contains the following:\n",
    "- `bundle_name`.trk\n",
    "\n",
    "  _original bundle tractography_ \n",
    "  \n",
    "  For each scalar feature (_optionally_):\n",
    "  - `scalar_name`.nii.gz\n",
    "\n",
    "     _original scalar image_\n",
    "     \n",
    "  - adjacency\\_`scalar_name`\\_r2.npy\n",
    "  \n",
    "    _scalar coefficent of determination adjacency matrix_\n",
    "  \n",
    "  For each cluster ($0...\\text{n_clusters}-1$):\n",
    "  - `model_name`\\_cluster\\_`cluster_id`.trk\n",
    "  \n",
    "    _uncleaned cluster tractography_\n",
    "    \n",
    "  - `model_name`\\_cluster\\_`cluster_id`\\_density\\_map.nii.gz\n",
    "  \n",
    "    _corresponding density map_\n",
    "\n",
    "  - `model_name`\\_idx.npy\n",
    "  \n",
    "    _array of length `n_streamlines` with each element representing the `cluster_id` assigned to streamline at that index_\n",
    "    \n",
    "  - `model_name`\\_info.pkl\n",
    "  \n",
    "    _pandas DataFrame containing metadata information about the model_\n",
    "    \n",
    "    - `subject` - model constructed using this subject's data\n",
    "    \n",
    "    - `session` - model constructed using this session's data\n",
    "    \n",
    "    - `bundle` - model constructed usng this bundle's data\n",
    "    \n",
    "    - `algorithm` - which clusetering algorithm used (e.g. KMeans); used to interpret score\n",
    "    \n",
    "    - `embedding dimensions` - if using spectral clustering, the embedding dimension $d$ for this model\n",
    "    \n",
    "    - `max n_clusters` - if using graspologic clustering, the recommended number of clusters $K$\n",
    "    \n",
    "    - `n_clusters selected` - if using graspologic clustering, the number of clusters identified in the data, this may be less than or equal to `max n_clusters`\n",
    "    \n",
    "    - `labels` - the labels for these clusters\n",
    "    \n",
    "    - `scores` - silhouette scores (for KMeans) or BIC (for GMM)\n",
    "    \n",
    "  - `model_name`\\_pairplot.png (_optionally_)\n",
    "  \n",
    "    _if graspologic spectral embedding, include the pairs plots showing the embeddings and cluster distributions_\n",
    "    \n",
    "  - `model_name`\\_silhouette\\_scores.npy (_optionally_)\n",
    "  \n",
    "    _if graspologic KMeans, the silhouette scores saved as array, so can be used to generate the group aggreate silhouette plots_\n",
    "    \n",
    "  - `model_name`\\_silhouette\\_scores.png (_optionally_)\n",
    "  \n",
    "    _if graspologic KMeans, the silhouette score for this individal model for $2...\\text{n_clusters}$. If searching for optimal $K$ then best to choose the largest number K representing the maximum number of expected subbundles for the bundle._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "\n",
    "Constants from pyAFQ and HCP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subbundle_model_analysis_utils import fetch_model_data\n",
    "from identify_subbundles import *\n",
    "import visualizations as viz\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('subbundle')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of pyAFQ bundle identifers\n",
    "BUNDLE_NAMES = [\n",
    "    'ATR_L', 'ATR_R',\n",
    "    'CGC_L', 'CGC_R',\n",
    "    'CST_L', 'CST_R',\n",
    "    'IFO_L', 'IFO_R',\n",
    "    'ILF_L', 'ILF_R',\n",
    "    'SLF_L', 'SLF_R',\n",
    "    'ARC_L', 'ARC_R',\n",
    "    'UNC_L', 'UNC_R',\n",
    "    'FA', 'FP'\n",
    "]\n",
    "\n",
    "# list of HCP test-retest subject identifiers\n",
    "SUBJECTS = [\n",
    "    '103818', '105923', '111312', '114823', '115320',\n",
    "    '122317', '125525', '130518', '135528', '137128',\n",
    "    '139839', '143325', '144226', '146129', '149337',\n",
    "    '149741', '151526', '158035', '169343', '172332',\n",
    "    '175439', '177746', '185442', '187547', '192439',\n",
    "    '194140', '195041', '200109', '200614', '204521',\n",
    "    '250427', '287248', '341834', '433839', '562345',\n",
    "    '599671', '601127', '627549', '660951', # '662551', \n",
    "    '783462', '859671', '861456', '877168', '917255'\n",
    "]\n",
    "\n",
    "# list of HCP test and retest session names\n",
    "SESSION_NAMES = ['HCP_1200', 'HCP_Retest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment and Model Metadata\n",
    "\n",
    "dictionary of information passed to helper functions\n",
    "\n",
    "- `metadata` dict:\n",
    "  - `metadata['experiment_name']`\n",
    "  - `metadata['experiment_output_dir']`\n",
    "  - `metadata['experiment_bundles']`\n",
    "  - `metadata['experiment_subjects']`\n",
    "  - `metadata['experiment_sessions']`\n",
    "  - `metadata['experiment_test_session']`\n",
    "  - `metadata['experiment_retest_session']`\n",
    "  - `metadata['experiment_range_n_clusters']`\n",
    "  - `metadata['experiment_bundle_dict']`\n",
    "  - `metadata['model_name']`\n",
    "  - `metadata['model_scalars']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_R\n",
      "['103818', '105923', '111312', '114823', '115320']\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "\n",
    "import random\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "metadata['experiment_name'] = 'MASE_FA_and_MD_Sklearn_KMeans'\n",
    "\n",
    "# TODO: output directory should exclude BUNDLE_NAME and be added at lower level helper where appropriate\n",
    "metadata['experiment_output_dir'] = join('subbundles', metadata['experiment_name'])\n",
    "\n",
    "# NOTE: right now just run one bundle at a time\n",
    "# BUNDLE_NAME = random.choice(BUNDLE_NAMES)\n",
    "# NOTE: Experiment was run for only SLF_L and SLF_R\n",
    "BUNDLE_NAME = random.choice(['SLF_L', 'SLF_R']) \n",
    "print(BUNDLE_NAME)\n",
    "metadata['experiment_bundles'] = [BUNDLE_NAME]\n",
    "\n",
    "# NOTE: Experiment was run for first five subjects\n",
    "metadata['experiment_subjects'] = SUBJECTS[:5] \n",
    "# metadata['experiment_subjects'] = random.sample(SUBJECTS, 5)\n",
    "print(metadata['experiment_subjects'])\n",
    "\n",
    "metadata['experiment_sessions'] = SESSION_NAMES\n",
    "metadata['experiment_test_session'] = metadata['experiment_sessions'][0]\n",
    "metadata['experiment_retest_session'] = metadata['experiment_sessions'][1]\n",
    "\n",
    "# NOTE: Experiment was run for 2-4 clusters\n",
    "metadata['experiment_range_n_clusters'] = [2, 3, 4] \n",
    "\n",
    "def make_bundle_dict(metadata):\n",
    "    \"\"\"\n",
    "    create a bundle dictionary object for the largest number of clusters\n",
    "    in the experiment\n",
    "    \"\"\"\n",
    "    bundle_dict = {}\n",
    "    \n",
    "    maximal_n_clusters = max(metadata['experiment_range_n_clusters'])\n",
    "    for bundle_name in metadata['experiment_bundles']:\n",
    "        bundle_name_prefix = bundle_name.split('_')[0]\n",
    "        \n",
    "        for cluster_id in range(maximal_n_clusters):\n",
    "            bundle_dict[bundle_name_prefix + '_' + str(cluster_id)] = {\"uid\" : cluster_id}\n",
    "        \n",
    "    return bundle_dict\n",
    "\n",
    "metadata['experiment_bundle_dict'] = make_bundle_dict(metadata)\n",
    "\n",
    "metadata['model_name'] = 'mase_kmeans_fa_r2_md_r2_is_mdf'\n",
    "metadata['model_scalars'] = [Scalars.DTI_FA, Scalars.DTI_MD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set up local directory and download necessary files for model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:subbundle:Download SLF_R data from HCP reliability study\n",
      "INFO:subbundle:Download SLF_R clustering models for K=[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "model_data = fetch_model_data(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. then we want to identify a consensus subject and appropriately relabel clusters; be able to evaluate using various algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build `cluster_info` dict:\n",
    "- `cluster_info[n_clusters]`\n",
    "  - `cluster_info[n_clusters]['consensus_subject']`\n",
    "  - `cluster_info[n_clusters]['centroids']`\n",
    "  - `cluster_info[n_clusters]['tractograms_filenames']`\n",
    "  - `cluster_info[n_clusters]['tractograms']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = get_cluster_info(metadata, BUNDLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relabel retest clusters based on consensus subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in algorithms:\n",
    "    match_retest_clusters(metadata, cluster_info, BUNDLE_NAME, algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "afq profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_afq_profiles = {}\n",
    "\n",
    "for n_clusters in metadata['experiment_range_n_clusters']:    \n",
    "     cluster_afq_profiles[n_clusters] = get_cluster_afq_profiles(\n",
    "        metadata, \n",
    "        BUNDLE_NAME, \n",
    "        n_clusters, \n",
    "        cluster_info[n_clusters]['consensus_subject']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. then we want to see individual (in subject space) **and** group (in MNI space):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. 1. anatomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 2. centriods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after generating cluster model studys published on aws s3 repository\n",
    "and having identifed consensus subject visualize centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show consensus subject centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.display_consensus_centroids(metadata, cluster_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*optionally* choose a subject to investigate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "viz.display_subject_centriods(metadata, cluster_info, random.choice(metadata['experiment_subjects']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Centroids Labeled by Streamline Count (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.display_streamline_count_centroids(metadata, cluster_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkout effects of different labeling algoritms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters Centroids Labeled by Best Weigheted Dice Coefficient\n",
    "\n",
    "**NOTE** this algorithm may 'collapse' multiple clusters into a single bundle as it will relabel cluster to consensus cluster with the highest overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.display_maxdice_centroids(metadata, cluster_info, BUNDLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters Centroids Labeled by Munkres (maximal trace) Weighted Dice Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.display_munkres_centroids(metadata, cluster_info, BUNDLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Centroids Labeled by MDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.display_mdf_centroids(metadata, cluster_info, BUNDLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 3. fa profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move to visualizations\n",
    "# TODO only does Munkres\n",
    "def display_cluster_profiles(metadata, cluster_info, cluster_afq_profiles, bundle_name):\n",
    "    \"\"\"\n",
    "    display the scalar profiles for each `n_clusters`\n",
    "    \"\"\"\n",
    "    from IPython.display import Image \n",
    "    from os.path import join\n",
    "    import itertools\n",
    "    \n",
    "    base_dir = join(metadata['experiment_output_dir'], bundle_name)\n",
    "\n",
    "    for n_clusters, scalar in itertools.product(metadata['experiment_range_n_clusters'], metadata['model_scalars']):\n",
    "        scalar_abr = scalar.split('.')[0]\n",
    "        viz.plot_cluster_reliability(\n",
    "            base_dir,\n",
    "            metadata['experiment_sessions'],\n",
    "            metadata['experiment_subjects'],\n",
    "            bundle_name,\n",
    "            scalar_abr,\n",
    "            cluster_afq_profiles[n_clusters][scalar],\n",
    "            model_data[bundle_name]['model_names'],\n",
    "            model_data[bundle_name]['cluster_names'],\n",
    "            n_clusters\n",
    "        )\n",
    "\n",
    "        for session in metadata['experiment_sessions']:\n",
    "            print(scalar, session, metadata['model_name'], n_clusters)\n",
    "            display(Image(filename=f\"{base_dir}/{session}_{metadata['model_name']}_{n_clusters}_clusters_{scalar_abr}_profile_ci.png\"))\n",
    "            \n",
    "display_cluster_profiles(metadata, cluster_info, cluster_afq_profiles, BUNDLE_NAME)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m = 1\n",
    "\n",
    "for n_clusters in metadata['experiment_range_n_clusters']:\n",
    "    for subject in cluster_afq_profiles[n_clusters][Scalars.DTI_MD].keys():\n",
    "        for session in cluster_afq_profiles[n_clusters][Scalars.DTI_MD][subject].keys():\n",
    "            for cluster in cluster_afq_profiles[n_clusters][Scalars.DTI_MD][subject][session].keys():\n",
    "                n = max(cluster_afq_profiles[n_clusters][Scalars.DTI_MD][subject][session][cluster])\n",
    "                if n > m:\n",
    "                    m = n\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 4. silhouette scores?\n",
    "\n",
    "3. 5. pair plots?\n",
    "\n",
    "see `subbundle_choose_k.pynb`\n",
    "\n",
    "TODO: merge into this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. profile tensors for each bundle and scalar\n",
    "\n",
    "      with each scalar profile tensor $KxNxMxS$, where:\n",
    "   \n",
    "      - $K$ is number of clusters,\n",
    "      - $N=44$ is number of subjects,\n",
    "      - $M=100$ is number of sampled streamline nodes, and \n",
    "      - $S=2$ is number of sessions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# want KxNxMxS - have NxSxKxM dict\n",
    "print('bundle', BUNDLE_NAME)\n",
    "print('clusters', N_CLUSTERS)\n",
    "print('subjects', list(cluster_afq_fa_profiles.keys()))\n",
    "subj = list(cluster_afq_fa_profiles.keys())[0]\n",
    "print('sessions', list(cluster_afq_fa_profiles[subj].keys()))\n",
    "ses = list(cluster_afq_fa_profiles[subj].keys())[0]\n",
    "print('cluster ids', list(cluster_afq_fa_profiles[subj][ses].keys()))\n",
    "cid = list(cluster_afq_fa_profiles[subj][ses].keys())[0]\n",
    "print('nodes', len(cluster_afq_fa_profiles[subj][ses][cid]))\n",
    "\n",
    "# could use lists to track keys and have multidimensional array\n",
    "# cluster_labels = [0 , n_clusters - 1]\n",
    "# subjects = [0 ... 43]\n",
    "# sessions = [0, 1]\n",
    "# nodes = [0 ... 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 DTI_FA.nii.gz (2, 5, 100, 2)\n",
      "2 DTI_MD.nii.gz (2, 5, 100, 2)\n",
      "3 DTI_FA.nii.gz (3, 5, 100, 2)\n",
      "3 DTI_MD.nii.gz (3, 5, 100, 2)\n",
      "4 DTI_FA.nii.gz (4, 5, 100, 2)\n",
      "4 DTI_MD.nii.gz (4, 5, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "def get_cluster_profile_tensor(cluster_afq_profiles, n_clusters, subjects, n_nodes, session_names):\n",
    "    \"\"\"\n",
    "    convert the cluster_afq_profile dict into ndarray\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    \n",
    "    tensor = np.zeros((n_clusters, len(subjects), n_nodes, len(session_names)))\n",
    "\n",
    "    for (subject, session, cluster_id, node_id) in itertools.product(subjects, session_names, range(n_clusters), range(n_nodes)):\n",
    "        tensor[cluster_id][subjects.index(subject)][node_id][session_names.index(session)] = cluster_afq_profiles[subject][session][cluster_id][node_id]\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "import itertools\n",
    "for n_clusters, scalar in itertools.product(metadata['experiment_range_n_clusters'], metadata['model_scalars']):  \n",
    "    profile_tensor = get_cluster_profile_tensor(\n",
    "        cluster_afq_profiles[n_clusters][scalar],\n",
    "        n_clusters,\n",
    "        metadata['experiment_subjects'],\n",
    "        100,\n",
    "        metadata['experiment_sessions']\n",
    "    )\n",
    "\n",
    "    print(n_clusters, scalar, profile_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
