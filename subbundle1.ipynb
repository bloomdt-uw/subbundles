{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subbundles Part 1: Data\n",
    "\n",
    "**Subbundle** - a subgroup of streamlines with a set of common properties\n",
    "\n",
    "Part 1: Download subject diffusion imaging data from dataset and run tractometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">**NOTE: Need to think about how to store results of expirements**</span>\n",
    "\n",
    "Currently:\n",
    "\n",
    "- Rerunning will overwrite files\n",
    "\n",
    "- Do not store any metadata about what was run\n",
    "\n",
    "- Save all artifacts and images\n",
    "\n",
    "- Does not output `tck` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <span style=\"color:red\">NOTE: several important variables are defined in `utils.py`</span>\n",
    "\n",
    "- `dataset_names` - these are supported datesets\n",
    "\n",
    "- `dataset_homes` - where datasets are stored under `AFQ_data`\n",
    "\n",
    "- `dataset_subjects` - which subjects to include for each dataset\n",
    "\n",
    "- `bundle_names` - which bundles to include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: assumes [MrTrix](https://mrtrix.readthedocs.io/en/latest/index.html) [`mrinfo`](https://mrtrix.readthedocs.io/en/latest/reference/commands/mrinfo.html#mrinfo) and [AWS-CLI](https://docs.aws.amazon.com/cli/index.html) [`aws`](https://docs.aws.amazon.com/cli/latest/reference/#aws) are installed</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "from AFQ import api\n",
    "import AFQ.data as afd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "0. Simulated data (*optional*)\n",
    "\n",
    "1. High Angular Resolution Diffusion MRI (`HARDI`) [<sup>1</sup>](https://figshare.com/articles/dataset/pyAFQ_Stanford_HARDI_tractography_and_mapping/3409882) [<sup>2</sup>](https://searchworks.stanford.edu/view/yx282xq2090)\n",
    "\n",
    "2. Human Connectome Project ([`HCP`](http://www.humanconnectomeproject.org)) [<sup>1</sup>](https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release)\n",
    "\n",
    "3. Human Connectome Project Retest (`HCP_retest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_test_retest = True\n",
    "\n",
    "dataset_name = 'HCP_retest'\n",
    "subjects = get_subjects_medium(dataset_name)\n",
    "print(dataset_name, subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm dwi data exists for each subject in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi_files = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    dwi_files[subject] = get_subject_session_dwi_file(dataset_name, get_subject_session_home(get_dmriprep_home(dataset_name), f'sub-{subject}'), f'sub-{subject}')\n",
    "\n",
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(pd.Series(dwi_files).to_frame('dwi file'))\n",
    "\n",
    "retrieve_data = False\n",
    "\n",
    "if (not os.path.isdir(get_dmriprep_home(dataset_name)) or\n",
    "    not np.all([os.path.exists(dwi_file) for dwi_file in dwi_files.values()])):\n",
    "    retrieve_data = True\n",
    "\n",
    "print('retrieve data:', retrieve_data)\n",
    "\n",
    "if retrieve_data:\n",
    "    fetch_data(dataset_name, subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HARDI\n",
    "\n",
    "#### Single subject single session\n",
    "\n",
    "<span style=\"color:blue\">**TODO: Download additional HARDI subjects?**</span>\n",
    "\n",
    "- If going to do this need to create fetcher and extend/overwrite `organize_stanford_data` to make BIDs compliant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HCP\n",
    "\n",
    "- <span style=\"color:red\">**Question: Are there certain subjects we have been using for quality control?**</span>\n",
    "\n",
    "- <span style=\"color:red\">**Question: Should look for subjects with certain properties?**</span>\n",
    "  \n",
    "    Like: \n",
    "\n",
    "  - test-retest?\n",
    "    \n",
    "  - twins?\n",
    "    \n",
    "  - demographic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HCP Single subject single session\n",
    "\n",
    "https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release\n",
    "\n",
    "- ##### <span style=\"color:red\">NOTE: Subject 100307 selected as exemplar subject from [HCP 1200 Subjects Data Release Reference Manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf)</span>\n",
    "\n",
    "- ##### <span style=\"color:red\">NOTE: Subject 103818 selected as exemplar test-retest subject from [Appendix 5: MR Data acquisition information for an exemplar subject](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Appendix_V.pdf) and we have processed data availabe on S3 </span>\n",
    "\n",
    "  - <span style=\"color:red\">**Question: Which Subject is 103818's twin?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HCP Retest Data\n",
    "\n",
    "> Retest datasets are available in the separate WU-Minn HCP Retest Data project\n",
    "\n",
    "> The retest data are released as a separate project to fully distinguish it from the first visit. Retest subjects retain the same Subject ID numbers as in the 1200 Subjects HCP project\n",
    "\n",
    "https://db.humanconnectome.org/data/projects/HCP_Retest\n",
    "\n",
    "> 45 Subjects with 3T Retest MRI and Behavioral Measures (46 subjects)\n",
    "\n",
    "> 46 subjects (all monozygotic twins, 21 twin pairs + 4 MZ twins without retest of co-twin) have 3T HCP protocol Retest data available.\n",
    "> - 36 Retest subjects have fully complete retest data for all 3T modalities: structural, rfMRI, tfMRI, and dfMRI\n",
    "\n",
    "> 7T data is available on 22 Retest subjects. \n",
    "\n",
    "**[Subjects with Retest MR data](https://db.humanconnectome.org/app/template/SubjectDashboard.vm?project=HCP_Retest&subjectGroupName=Subjects%20with%20retest%20MR%20data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate DWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dwi_niis = []\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dwi_niis.append(nib.load(dwi_files[subject]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check header\n",
    "\n",
    "<span style=\"color:blue\">**TODO: Confirm the [reference space](https://nipy.org/nibabel/coordinate_systems.html#the-scanner-subject-reference-space) is RAS+**</span>\n",
    "\n",
    "- HARDI and HCP hemispheres are reversed; could be related to reference space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_header = False\n",
    "\n",
    "if check_header:\n",
    "    for dwi_nii in subject_dwi_niis:\n",
    "#         !mrinfo {dwi_file}\n",
    "        print(dwi_nii.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display DWI Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dwi = False\n",
    "\n",
    "if display_dwi:\n",
    "    for subject, dwi_nii in zip(subjects, subject_dwi_niis):\n",
    "        if compare_test_retest:\n",
    "            # by definition same subject should exist in HCP dataset\n",
    "            hcp_dwi_file = get_subject_session_dwi_file(get_subject_session_home(get_dmriprep_home('HCP'), f'sub-{subject}'), f'sub-{subject}')\n",
    "            hcp_dwi_nii = nib.load(hcp_dwi_file)\n",
    "            display_dwi_slice(f'HCP {subject}', hcp_dwi_nii)\n",
    "\n",
    "        display_dwi_slice(f'{dataset_name} {subject}', dwi_nii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tract Profiles\n",
    "\n",
    "1. Single individual from HARDI dataset\n",
    "\n",
    "2. Multiple individuals from HARDI dataset\n",
    "\n",
    "3. Multiple individuals from HCP dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: Run and compare deterministic and probabilistic tractography**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: Custom Bundles (SLF Subbundles, Callosum Bundle)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myafq = get_afq(dataset_name)\n",
    "\n",
    "display(myafq.data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: Number of individuals depends on the number of subject in `bids_path`</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "\n",
    "Confirm AFQ derivatives exist for each subject in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: Processed HCP data is available from S3</span>\n",
    "\n",
    "https://s3.console.aws.amazon.com/s3/buckets/profile-hcp-west?region=us-west-2&prefix=hcp_reliability/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any subjects in our subject list that are not already in afq derivatives\n",
    "segment = (set(subjects) > set(myafq.subjects))\n",
    "\n",
    "print('segment:', segment)\n",
    "\n",
    "if not segment: \n",
    "    results_dirs = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        iloc = get_iloc(myafq, subject)\n",
    "        results_dirs[subject] = myafq.data_frame.iloc[iloc]['results_dir']\n",
    "\n",
    "    with pd.option_context('display.max_colwidth', -1):\n",
    "        display(pd.Series(results_dirs).to_frame('results directory'))\n",
    "\n",
    "    # NOTE: this logic is incorrect -- checking the afq derivatives results dirs is insufficient\n",
    "    # must check whether contain files...\n",
    "    segment = (not np.all([os.path.isdir(results_dir) for results_dir in results_dirs.values()]))\n",
    "\n",
    "print('segment:', segment)\n",
    "\n",
    "if segment:\n",
    "    if dataset_name == 'HARDI':\n",
    "        myafq.export_all()\n",
    "    else:\n",
    "        for subject in subjects:\n",
    "            subject_base_dir = op.join(get_afq_home(dataset_name), f'sub-{subject}')\n",
    "\n",
    "            # Create afq derivatives and subject directories if do not exist\n",
    "            os.makedirs(subject_base_dir, exist_ok=True)\n",
    "\n",
    "            hcp_s3_url = get_hcp_s3_url(dataset_name, subject)\n",
    "\n",
    "            # fetch hcp data from s3\n",
    "            !aws s3 cp {hcp_s3_url} {subject_base_dir} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualization\n",
    "\n",
    "optional quality control check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: Left and Right hemispheres appear flipped between HARDI and HCP**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bundles = False\n",
    "\n",
    "if show_bundles:\n",
    "    for subject in subjects:\n",
    "        loc = get_iloc(myafq, subject)\n",
    "        bundle_html = myafq.viz_bundles(export=True, n_points=50)\n",
    "        plotly.io.show(bundle_html[loc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tract Profiles\n",
    "\n",
    "##### <span style=\"color:blue\">TODO: `plot_tract_profiles` failing with `KeyError` for `HCP` and `HCP_retest`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tract_profiles = False\n",
    "\n",
    "if show_tract_profiles:\n",
    "    myafq.plot_tract_profiles()\n",
    "    display(Image(filename=myafq.data_frame['tract_profiles_viz'][0][0]+'.png', width = 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
