{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subbundles Part 2: Streamlines\n",
    "\n",
    "**Subbundle** - a subgroup of streamlines with a set of common properties\n",
    "\n",
    "Part 2: get `streamlines` and `affine`\n",
    "\n",
    "##### <span style=\"color:red\">NOTE: Part 2 is more standalone quality control; nothing from this part is saved or used by any other parts</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "import nibabel as nib\n",
    "from dipy.io.streamline import load_tractogram\n",
    "from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "from dipy.tracking.streamline import set_number_of_points\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from AFQ import api\n",
    "import AFQ.data as afd\n",
    "from AFQ.viz.fury_backend import visualize_volume\n",
    "from AFQ.viz.fury_backend import visualize_bundles\n",
    "\n",
    "import tempfile\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from dipy.viz import window, actor\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFQ (from Part 1)\n",
    "\n",
    "Instantiate AFQ object: `myafq` for desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_test_retest = True\n",
    "\n",
    "test_retest_dir = 'HCP_test_retest'\n",
    "test_retest_sessions = ['test', 'retest']\n",
    "test_retest_names = ['HCP', 'HCP_retest']\n",
    "\n",
    "# dataset_name = 'HCP'\n",
    "dataset_name = 'HCP_retest'\n",
    "\n",
    "# subjects = get_subjects(dataset_name)\n",
    "subjects = get_subjects_small(dataset_name)\n",
    "# subjects = get_subjects_medium(dataset_name)\n",
    "\n",
    "if compare_test_retest:\n",
    "    sub_dir = test_retest_dir\n",
    "    \n",
    "    print('HCP')\n",
    "    myafq_test = get_afq('HCP')\n",
    "    display(myafq_test.data_frame)\n",
    "    \n",
    "    print('HCP_retest')\n",
    "    myafq_retest = get_afq('HCP_retest')\n",
    "    display(myafq_retest.data_frame)\n",
    "else:\n",
    "    sub_dir = dataset_name\n",
    "    \n",
    "    print(dataset_name)\n",
    "    myafq = get_afq(dataset_name)\n",
    "    display(myafq.data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bundles\n",
    "\n",
    "1. SLF\n",
    "2. Corpus Callosum\n",
    "3. Novel bundle\n",
    "4. Whole brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SLF - *superior longitudinal fasciculus* (reproduce)\n",
    "\n",
    "- Grotheer, M., Zhen, Z., Lerma-Usabiaga, G., & Grill-Spector, K. (2019). Separate lanes for adding and reading in the white matter highways of the human brain. Nature communications, 10(1), 1-14.\n",
    "\n",
    "  https://www.nature.com/articles/s41467-019-11424-1\n",
    "\n",
    "\n",
    "- Schurr, R., Zelman, A., & Mezer, A. A. (2020). Subdividing the superior longitudinal fasciculus using local quantitative MRI. NeuroImage, 208, 116439.\n",
    "\n",
    "  https://www.sciencedirect.com/science/article/pii/S1053811919310304\n",
    "  \n",
    "\n",
    "- De Schotten, M. T., Dellâ€™Acqua, F., Forkel, S., Simmons, A., Vergani, F., Murphy, D. G., & Catani, M. (2011). A lateralized brain network for visuo-spatial attention. Nature Precedings, 1-1.\n",
    "\n",
    "  https://www.nature.com/articles/npre.2011.5549.1\n",
    "  https://www.researchgate.net/publication/281573090_A_lateralized_brain_network_for_spatial_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: SLF subbundle tractometry (in part 1)**</span>\n",
    "\n",
    "Represent anatomical correlate\n",
    "\n",
    "- define ROIs [Issue #600](https://github.com/yeatmanlab/pyAFQ/issues/600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: By default use SLF bundles, otherwise:</span>\n",
    "\n",
    "- **Change `bundle_names`**\n",
    "\n",
    "  - pyAFQ segmentation was run with default bundles. To determine valid names can either:\n",
    "  \n",
    "     - refer to [documentation](https://yeatmanlab.github.io/pyAFQ/), or \n",
    "     \n",
    "     - inspect the `myafq.bundle_dict` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if compare_test_retest:\n",
    "#     bundle_names = [*myafq_retest.bundle_dict]\n",
    "# else:\n",
    "#     bundle_names = [*myafq.bundle_dict]\n",
    "\n",
    "# bundle_names = ['SLF_L', 'SLF_R']\n",
    "# bundle_names = ['ARC_L', 'ARC_R', 'CST_L', 'CST_R', 'FP'] \n",
    "bundle_names = ['SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'CST_L', 'CST_R', 'FP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Corpus callosum tract profiles (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: Corpus callosum tractometry (in part 1)**</span>\n",
    "\n",
    "- define ROIs\n",
    "\n",
    "  - use midsaggital inclusion ROI and through midline\n",
    "  \n",
    "  - union of all callosum bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Novel bundles (predictive)\n",
    "\n",
    "Bundles where results are less established and more speculative\n",
    "\n",
    "<span style=\"color:blue\">**TODO: select existing bundles**</span>\n",
    "\n",
    "- <span style=\"color:red\">**Question: what bundles choose?**</span>\n",
    "\n",
    "  - Are there other bundles that would be ideal candidates? If so, why?\n",
    "  \n",
    "  - What does literature say?\n",
    "  \n",
    "  - Could be greedy run on all bundles defined by RECO or Waypoint ROI\n",
    "\n",
    "- <span style=\"color:red\">**Question: are there any bundles that do not result in subbundles?**</span>\n",
    "\n",
    "  - Or does this approach always subdivide?\n",
    "  \n",
    "    - For example if recursively use outputs as inputs, will there always be more subbundles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Whole Brain Tractometry\n",
    "\n",
    "<span style=\"color:blue\">**TODO: Run on whole brain tractometry**</span>\n",
    "\n",
    "- See whether reproduce same top level bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tractogram Files\n",
    "\n",
    "Name of tractogram file: `tg_fname` used for importing initial streamlines. Streamlines may represent whole brain or some subset (bundle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_fnames = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    tg_fnames[subject] = {}\n",
    "\n",
    "    if compare_test_retest:\n",
    "        loc_test  = get_iloc(myafq_test, subject)\n",
    "        loc_retest = get_iloc(myafq_retest, subject)\n",
    "    else:\n",
    "        loc = get_iloc(myafq, subject)\n",
    "        \n",
    "    for bundle_name in bundle_names:\n",
    "        tg_fnames[subject][bundle_name] = {}\n",
    "        \n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions, [myafq_test, myafq_retest], [loc_test, loc_retest])\n",
    "        else:\n",
    "            iterables = zip([sub_dir], [dataset_name], [myafq], [loc])\n",
    "\n",
    "        for name, ses, myafq, loc in iterables:\n",
    "            tg_fnames[subject][bundle_name][ses] = get_tractogram_filename(myafq, bundle_name, loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Tractogram Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_header = False\n",
    "\n",
    "if check_header:\n",
    "    for subject in subjects:\n",
    "        for bundle_name in bundle_names:\n",
    "            if compare_test_retest:\n",
    "                tg_fname = tg_fnames[subject][bundle_name]['retest']\n",
    "            else:    \n",
    "                tg_fname = tg_fnames[subject][bundle_name][dataset_name]\n",
    "            \n",
    "            print(sub_dir, subject, bundle_name, nib.streamlines.load(tg_fname).header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tractograms = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    tractograms[subject] = {}\n",
    "    \n",
    "    for bundle_name in bundle_names:\n",
    "        tractograms[subject][bundle_name] = {}\n",
    "\n",
    "        if compare_test_retest:\n",
    "            iterables = zip(test_retest_names, test_retest_sessions)\n",
    "        else:\n",
    "            iterables = zip([sub_dir''], [dataset_name])\n",
    "\n",
    "        for name, ses in iterables:\n",
    "            tg_fname = tg_fnames[subject][bundle_name][ses]\n",
    "            tractograms[subject][bundle_name][ses] = load_tractogram(tg_fname, 'same')\n",
    "\n",
    "            \n",
    "tg_df = pd.DataFrame.from_dict(\n",
    "    {(i,j,k): [len(tractograms[i][j][k].streamlines), tractograms[i][j][k].affine, tg_fnames[i][j][k]] for i in tractograms.keys() for j in tractograms[i].keys() for k in tractograms[i][j].keys()}, \n",
    "    orient='index', \n",
    "    columns=['number of streamlines', 'affine', 'tratogram files']\n",
    ")\n",
    "\n",
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(tg_df)\n",
    "\n",
    "os.makedirs(op.join('subbundles', sub_dir), exist_ok=True)\n",
    "f_name = op.join('subbundles', sub_dir, f'tractogram_info.csv')\n",
    "print(f_name)\n",
    "tg_df.to_csv(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC: Bundle Visualization (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bundle Streamlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: Left and Right hemispheres appear flipped between HARDI and HCP**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_bundle_streamlines = False\n",
    "\n",
    "if show_bundle_streamlines:    \n",
    "    for subject in subjects:\n",
    "        if compare_test_retest:\n",
    "            loc_test  = get_iloc(myafq_test, subject)\n",
    "            loc_retest = get_iloc(myafq_retest, subject)\n",
    "        else:\n",
    "            loc = get_iloc(myafq, subject)\n",
    "            \n",
    "        for bundle_name in bundle_names:\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions, [myafq_test, myafq_retest], [loc_test, loc_retest])\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name], [myafq], [loc])\n",
    "\n",
    "            for name, ses, myafq, loc in iterables:\n",
    "                fname = tempfile.NamedTemporaryFile().name + '.png'\n",
    "                \n",
    "                volume, color_by_volume = myafq._viz_prepare_vols(\n",
    "                    myafq.data_frame.iloc[loc],\n",
    "                    volume=None,\n",
    "                    xform_volume=False,\n",
    "                    color_by_volume=None,\n",
    "                    xform_color_by_volume=False\n",
    "                )\n",
    "                \n",
    "                scene = visualize_volume(\n",
    "                    volume,\n",
    "                    interact=False,\n",
    "                    inline=False\n",
    "                )\n",
    "\n",
    "                tractogram = tractograms[subject][bundle_name][ses]\n",
    "                sft = StatefulTractogram.from_sft(tractogram.streamlines, tractogram)\n",
    "                sft.to_vox()\n",
    "                visualize_bundles(sft, figure=scene, interact=interact)\n",
    "\n",
    "                print(name, subject, bundle_name, ses, 'streamlines')\n",
    "                window.record(scene, out_path=fname, size=(300, 300))\n",
    "                display(Image(filename=fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bundle Tract Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_bundle_tract_profiles = True\n",
    "\n",
    "if show_bundle_tract_profiles:\n",
    "    for subject in subjects:\n",
    "        if compare_test_retest:\n",
    "            loc_test  = get_iloc(myafq_test, subject)\n",
    "            loc_retest = get_iloc(myafq_retest, subject)\n",
    "        else:\n",
    "            loc = get_iloc(myafq, subject)\n",
    "\n",
    "        for bundle_name in bundle_names:\n",
    "            if compare_test_retest:\n",
    "                make_dirs(myafq_retest, sub_dir, bundle_name, subjects)\n",
    "                target_dir = get_dir_name(myafq_retest, sub_dir, bundle_name, loc_retest)\n",
    "                scalars = myafq_retest.scalars\n",
    "            else:\n",
    "                make_dirs(myafq, sub_dir, bundle_name, subjects)\n",
    "                target_dir = get_dir_name(myafq, sub_dir, bundle_name, loc)\n",
    "                scalars = myafq.scalars\n",
    "            \n",
    "            for scalar_name in scalars:\n",
    "                truncated_name = scalar_name.split('_')[-1]\n",
    "\n",
    "                profiles = []\n",
    "                \n",
    "                if compare_test_retest:\n",
    "                    iterables = zip(test_retest_names, test_retest_sessions, [myafq_test, myafq_retest], [loc_test, loc_retest])\n",
    "                else:\n",
    "                    iterables = zip([sub_dir], [dataset_name], [myafq], [loc])\n",
    "\n",
    "                for name, ses, myafq, loc in iterables:\n",
    "                    scalar_data = nib.load(get_scalar_filename(myafq, scalar_name, loc)).get_fdata()\n",
    "                    \n",
    "                    if len(tractograms[subject][bundle_name][ses].streamlines) == 0:\n",
    "                        profiles.append(np.zeros(100))\n",
    "                    else:\n",
    "                        profiles.append(afq_profile(\n",
    "                            scalar_data,\n",
    "                            tractograms[subject][bundle_name][ses].streamlines,\n",
    "                            tractograms[subject][bundle_name][ses].affine,\n",
    "                            weights=gaussian_weights(tractograms[subject][bundle_name][ses].streamlines)\n",
    "                        ))\n",
    "\n",
    "                if compare_test_retest:\n",
    "                    print('test-retest bundle profile correlation:')\n",
    "                    # Calculate Pearson correlations between profiles (test-retest reliability)\n",
    "                    test_retest_corr_matrix = pd.DataFrame(zip(*profiles), columns=test_retest_sessions).corr()\n",
    "                \n",
    "                    # select only the upper triangle off diagonals of the correlation matrix\n",
    "                    test_retest_corr = pd.Series(test_retest_corr_matrix.where(np.triu(np.ones(test_retest_corr_matrix.shape), 1).astype(np.bool)).stack(), name='corr')\n",
    "                    \n",
    "                    display(test_retest_corr)\n",
    "                    \n",
    "                    # save the correlation\n",
    "                    f_name = op.join(target_dir, f'{truncated_name}_trt_tract_profile_corr.csv')\n",
    "                    print(f_name)\n",
    "                    test_retest_corr.to_csv(f_name)    \n",
    "                \n",
    "                plt.figure()\n",
    "                \n",
    "                if compare_test_retest and not test_retest_corr.empty:\n",
    "                    plt.title(f'{dataset_name} {subject} {bundle_name} {scalar_name} tract profiles\\ncorrelation {test_retest_corr.iloc[0]:.5f}')\n",
    "                else:\n",
    "                    plt.title(f'{dataset_name} {subject} {bundle_name} {scalar_name} tract profiles')\n",
    "                \n",
    "                if compare_test_retest:\n",
    "                    iterables = zip(test_retest_sessions, profiles)\n",
    "                else:\n",
    "                    iterables = zip([dataset_name], profiles)\n",
    "                    \n",
    "                for ses, profile in iterables:\n",
    "                    if ses == 'retest':\n",
    "                        plt.plot(profile, c='k', linestyle='dashed', label=ses)\n",
    "                    else:\n",
    "                        plt.plot(profile, c='k', label=ses)\n",
    "                                  \n",
    "                plt.xlabel('node index')\n",
    "                plt.ylabel(f'{scalar_name} values')\n",
    "                plt.legend()\n",
    "                if compare_test_retest:\n",
    "                    f_name = op.join(target_dir, f'{truncated_name}_trt_tract_profile.png')\n",
    "                else:\n",
    "                    f_name = op.join(target_dir, f'{truncated_name}_tract_profile.png')\n",
    "                print(f_name)\n",
    "                plt.savefig(f_name)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC: Streamline Metrics/Statisitical Measurements\n",
    "\n",
    "<span style=\"color:blue\">**TODO: outliers, mean, and variation**</span>\n",
    "\n",
    "- See [Streamline analysis and connectivity](https://dipy.org/documentation/1.3.0./examples_index/#streamline-analysis-and-connectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Distribution of Streamline Length (voxel units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_min_max_ref_streamline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bundle_voxel_freq = False or show_min_max_ref_streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_bundle_voxel_freq:\n",
    "    voxel_freqs = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        voxel_freqs[subject] = {}\n",
    "\n",
    "        for bundle_name in bundle_names:\n",
    "            voxel_freqs[subject][bundle_name] = {}\n",
    "\n",
    "            if compare_test_retest:\n",
    "                iterables = test_retest_sessions\n",
    "            else:\n",
    "                iterables = [dataset_name]\n",
    "\n",
    "            for ses in iterables:\n",
    "                voxel_freqs[subject][bundle_name][ses] = [len(streamline) for streamline in tractograms[subject][bundle_name][ses].streamlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_bundle_voxel_freq:\n",
    "    for subject in subjects:\n",
    "        for bundle_name in bundle_names:\n",
    "            plt.figure()\n",
    "            plt.title(f'{dataset_name} {subject} {bundle_name} Streamline Voxel Frequency')\n",
    "\n",
    "            voxel_freq = voxel_freqs[subject][bundle_name]\n",
    "\n",
    "            if compare_test_retest:\n",
    "                plt.hist((voxel_freq['test'], voxel_freq['retest']), bins=range(min(voxel_freq['retest'])-min(voxel_freq['retest'])%50, max(voxel_freq['retest'])+50-max(voxel_freq['retest'])%50, 50), label=('test', 'retest'))\n",
    "            else:\n",
    "                plt.hist(voxel_freq[dataset_name], bins=range(min(voxel_freq[dataset_name])-min(voxel_freq[dataset_name])%50, max(voxel_freq[dataset_name])+50-max(voxel_freq[dataset_name])%50, 50))\n",
    "            \n",
    "            plt.xlabel('length')\n",
    "            plt.ylabel('num streamlines')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Various Visualizations for Min, Max, Mean Streamlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question: What is appropriate reference?**</span>\n",
    "    \n",
    "- First pass, calculate a streamline on mean position of all streamlines in bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create mean position (centriod) streamline\n",
    "\n",
    "Resample streamlines so can compute the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_min_max_ref_streamline:\n",
    "    reference_streamlines = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        reference_streamlines[subject] = {}\n",
    "        for bundle_name in bundle_names:\n",
    "            reference_streamlines[subject][bundle_name] = {}\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions)\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name])\n",
    "\n",
    "            for name, ses in iterables:\n",
    "                reference_streamlines[subject][bundle_name][ses] = np.mean(set_number_of_points(tractograms[subject][bundle_name][ses].streamlines, 100), axis=0)\n",
    "\n",
    "    display(pd.DataFrame.from_dict(\n",
    "        {(i,j,k): len(reference_streamlines[i][j][k]) for i in reference_streamlines.keys() for j in reference_streamlines[i].keys() for k in reference_streamlines[i][j].keys()}, \n",
    "        orient='index', \n",
    "        columns=['reference streamline n_points']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify min and max streamlines by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_min_max_ref_streamline:\n",
    "    max_streamlines = {}\n",
    "    min_streamlines = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        max_streamlines[subject] = {}\n",
    "        min_streamlines[subject] = {}\n",
    "        for bundle_name in bundle_names:\n",
    "            max_streamlines[subject][bundle_name] = {}\n",
    "            min_streamlines[subject][bundle_name] = {}\n",
    "\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions)\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name])\n",
    "\n",
    "            for name, ses in iterables:\n",
    "                max_streamlines[subject][bundle_name][ses] = tractograms[subject][bundle_name][ses].streamlines[np.argmax(voxel_freqs[subject][bundle_name][ses])]\n",
    "                min_streamlines[subject][bundle_name][ses] = tractograms[subject][bundle_name][ses].streamlines[np.argmin(voxel_freqs[subject][bundle_name][ses])]\n",
    "\n",
    "    display(pd.DataFrame.from_dict(\n",
    "        {(i,j,k): [len(min_streamlines[i][j][k]), len(max_streamlines[i][j][k])] for i in reference_streamlines.keys() for j in reference_streamlines[i].keys() for k in reference_streamlines[i][j].keys()}, \n",
    "        orient='index', \n",
    "        columns=['min n_points', 'max n_points']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resample the min and max streamlines\n",
    "\n",
    "So that they both have the same number of points per streamline\n",
    "\n",
    "[`set_number_of_points`](https://dipy.org/documentation/1.3.0./reference/dipy.segment/#dipy.segment.benchmarks.bench_quickbundles.set_number_of_points)\n",
    "\n",
    "Change the number of points of streamlines in order to obtain `nb_points-1` segments of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_min_max_ref_streamline:\n",
    "    sampled_max_streamlines = {}\n",
    "    sampled_min_streamlines = {}\n",
    "\n",
    "\n",
    "    for subject in subjects:\n",
    "        sampled_max_streamlines[subject] = {}\n",
    "        sampled_min_streamlines[subject] = {}\n",
    "        for bundle_name in bundle_names:\n",
    "            sampled_max_streamlines[subject][bundle_name] = {}\n",
    "            sampled_min_streamlines[subject][bundle_name] = {}\n",
    "\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions)\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name])\n",
    "\n",
    "            for name, ses in iterables:\n",
    "                sampled_max_streamlines[subject][bundle_name][ses] = set_number_of_points(max_streamlines[subject][bundle_name][ses], 100)\n",
    "                sampled_min_streamlines[subject][bundle_name][ses] = set_number_of_points(min_streamlines[subject][bundle_name][ses], 100)\n",
    "\n",
    "    display(pd.DataFrame.from_dict(\n",
    "        {(i,j,k): [len(sampled_min_streamlines[i][j][k]), len(sampled_max_streamlines[i][j][k])] for i in reference_streamlines.keys() for j in reference_streamlines[i].keys() for k in reference_streamlines[i][j].keys()}, \n",
    "        orient='index', \n",
    "        columns=['sampled min n_points', 'sampled max n_points']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC: Streamline Visualization\n",
    "\n",
    "Plot the min, max, and mean streamlines in RAS+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_min_max_ref_streamline:\n",
    "    for subject in subjects:\n",
    "        for bundle_name in bundle_names:\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions)\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name])\n",
    "\n",
    "            for name, ses in iterables:\n",
    "                maxlen = max_streamlines[subject][bundle_name][ses]\n",
    "                minlen = min_streamlines[subject][bundle_name][ses]\n",
    "                mean = reference_streamlines[subject][bundle_name][ses]\n",
    "                fig = plt.figure()\n",
    "                ax = Axes3D(fig)\n",
    "                plt.title(f'{name} {subject} {bundle_name} {ses} Streamlines (RAS mm)')\n",
    "                ax.scatter3D(maxlen[:,0], maxlen[:,1], maxlen[:,2], c='tab:green', label='max')\n",
    "                ax.scatter3D(minlen[:,0], minlen[:,1], minlen[:,2], c='g', label='min')\n",
    "                ax.scatter3D(mean[:,0], mean[:,1],mean[:,2], c='tab:red', label='mean')\n",
    "                plt.legend()\n",
    "                ax.set_xlabel('transverse/axial') # left (-) / right (+)\n",
    "                ax.set_ylabel('sagittal') # back (-) / forward (+)\n",
    "                ax.set_zlabel('coronal') # bottom (-) / top (+)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO: determine camera `positon` and `focal_point`**</span>\n",
    "\n",
    "<span style=\"color:red\">**Question: What cooridnate system is volume in? What is demension of the volume? and How does `visualize_bundles` get horizontal slice perspective?**</span>\n",
    "\n",
    "- Given the name of of `trk` file *RASMM* should be in subject space with RAS MM coordinates. Another indication is that the coordinate axes are negative, which wouldn't happen for voxel space.\n",
    "\n",
    "```\n",
    "scene.elevation(90)\n",
    "scene.set_camera(view_up=(0.0, 0.0, 1.0))\n",
    "```\n",
    "\n",
    "```\n",
    "print(scene.get_camera())\n",
    "scene.set_camera(position=(0, 0, 1), focal_point=(1, 0, 0))\n",
    "print(scene.get_camera())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_min_max_ref_streamline:\n",
    "    for subject in subjects:\n",
    "        for bundle_name in bundle_names:\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions)\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name])\n",
    "\n",
    "            for name, ses in iterables:\n",
    "                maxlen = max_streamlines[subject][bundle_name][ses]\n",
    "                minlen = min_streamlines[subject][bundle_name][ses]\n",
    "                mean = reference_streamlines[subject][bundle_name][ses]\n",
    "\n",
    "                fname = tempfile.NamedTemporaryFile().name + '.png'\n",
    "                scene = window.Scene()\n",
    "                # scene.add(actor.streamtube([maxlen, minlen], linewidth=0.5))\n",
    "                scene.add(actor.point(maxlen, window.colors.green, point_radius=1))\n",
    "                scene.add(actor.point(minlen, window.colors.green, point_radius=1))\n",
    "                scene.add(actor.point(mean, window.colors.red, point_radius=1))\n",
    "\n",
    "                if interact:\n",
    "                    window.show(scene, title=f'{name} {subject} {bundle_name} {ses} min/max/ref streamlines', size=(300, 300))\n",
    "\n",
    "                print(name, subject, bundle_name, ses, 'min/max/ref streamlines')\n",
    "                window.record(scene, out_path=fname, size=(300, 300))\n",
    "                display(Image(filename=fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question: Why downsample streamlines? Why choose `n=100`?**</span>\n",
    "\n",
    "- Data reduction/runtime performance? Simplify calculations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_min_max_ref_streamline:\n",
    "    for subject in subjects:\n",
    "        for bundle_name in bundle_names:\n",
    "            if compare_test_retest:\n",
    "                iterables = zip(test_retest_names, test_retest_sessions)\n",
    "            else:\n",
    "                iterables = zip([sub_dir], [dataset_name])\n",
    "\n",
    "            for name, ses in iterables:\n",
    "                maxlen = sampled_max_streamlines[subject][bundle_name][ses]\n",
    "                minlen = sampled_min_streamlines[subject][bundle_name][ses]\n",
    "                mean = reference_streamlines[subject][bundle_name][ses]\n",
    "\n",
    "                fname = tempfile.NamedTemporaryFile().name + '.png'\n",
    "                scene = window.Scene()\n",
    "                # scene.add(actor.streamtube([sampledmaxlen, sampledminlen], linewidth=0.5))\n",
    "                scene.add(actor.point(maxlen, window.colors.green, point_radius=1))\n",
    "                scene.add(actor.point(minlen, window.colors.green, point_radius=1))\n",
    "                scene.add(actor.point(mean, window.colors.red, point_radius=1))\n",
    "\n",
    "                if interact:\n",
    "                    window.show(scene, title=f'{name} {subject} {bundle_name} {ses} min/max/ref sampled streamlines', size=(300, 300))\n",
    "\n",
    "                print(name, subject, bundle_name, ses, 'min/max/ref sampled streamlines')\n",
    "                window.record(scene, out_path=fname, size=(300, 300))\n",
    "                display(Image(filename=fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamline Registration (Move to part 7)\n",
    "\n",
    "See [Streamline-based Registration](https://dipy.org/tutorials/#id10)\n",
    "\n",
    "Register two bundles from two different subjects directly in the space of streamlines\n",
    "\n",
    "This will be useful for multisubject comparison\n",
    "\n",
    "<span style=\"color:red\">**Question: Which subject is `static` and which is `moving`? Is there some similar notion to MNI space?**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Streamline-based Linear Registration (SLR)](https://dipy.org/documentation/1.3.0./examples_built/bundle_registration/)\n",
    "\n",
    "<span style=\"color:blue\">**TODO: Provide some context and description of problem**</span>\n",
    "\n",
    "- https://www.sciencedirect.com/science/article/pii/S1053811915003961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
