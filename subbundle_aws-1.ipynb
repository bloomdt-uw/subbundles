{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expirements\n",
    "* MASE\n",
    "  - KMeansClustering\n",
    "  - GMM\n",
    "* MASE with Laplace Transform\n",
    "* OMNI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE `662551` not in `HCP_1200`**\n",
    "\n",
    "```\n",
    "s3://profile-hcp-west/hcp_reliability/single_shell/hcp_1200_afq/\n",
    "s3://profile-hcp-west/hcp_reliability/single_shell/hcp_1200_afq_CSD/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\n",
    "    '103818', '105923', '111312', '114823', '115320',\n",
    "    '122317', '125525', '130518', '135528', '137128',\n",
    "    '139839', '143325', '144226', '146129', '149337',\n",
    "    '149741', '151526', '158035', '169343', '172332',\n",
    "    '175439', '177746', '185442', '187547', '192439',\n",
    "    '194140', '195041', '200109', '200614', '204521',\n",
    "    '250427', '287248', '341834', '433839', '562345',\n",
    "    '599671', '601127', '627549', '660951', # '662551', \n",
    "    '783462', '859671', '861456', '877168', '917255'\n",
    "]\n",
    "session_names = ['HCP_1200', 'HCP_Retest']\n",
    "# bundle_names = ['SLF_L']\n",
    "# bundle_names = ['SLF_L', 'SLF_R']\n",
    "bundle_names = ['SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'CST_L', 'CST_R']\n",
    "# n_clusters = [2, 3, 4]\n",
    "n_clusters = [2,4,6,8,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# args = list(itertools.product(subjects, session_names, bundle_names))\n",
    "args = list(itertools.product(subjects, session_names, bundle_names, n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('103818', 'HCP_1200', 'SLF_L', 2),\n",
       " ('103818', 'HCP_1200', 'SLF_L', 4),\n",
       " ('103818', 'HCP_1200', 'SLF_L', 6),\n",
       " ('103818', 'HCP_1200', 'SLF_L', 8),\n",
       " ('103818', 'HCP_1200', 'SLF_L', 10)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful for when individual jobs fail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def job_args(ids):\n",
    "    import numpy as np\n",
    "    \n",
    "    return list(map(tuple, np.array(args)[ids].tolist()))\n",
    "    \n",
    "# TODO use knot to determine failed and running jobs\n",
    "# may want to stop running and rerun these with larger memory and disk allocations\n",
    "def failed_jobs():\n",
    "    import numpy as np\n",
    "    # failed_job_ids = [19,22,81,101,148,149]\n",
    "    failed_job_ids = [19,22,81,101]\n",
    "#     print(len(failed_job_ids))\n",
    "    return job_args(np.array(failed_job_ids)+8)\n",
    "\n",
    "def running_jobs():\n",
    "    import numpy as np\n",
    "    running_job_ids = [1,3,13,15,35,59,83,117,119,131,137,139,141,143,147,163]\n",
    "#     print(len(running_job_ids))\n",
    "    return job_args(np.array(running_job_ids)+8)\n",
    "\n",
    "def rerun_targets():\n",
    "    import numpy as np\n",
    "    # failed twice\n",
    "    targets = list(map(tuple, np.array([*failed_jobs(), *running_jobs()])[[0,5,6,7,10,12]]))\n",
    "    return targets\n",
    "\n",
    "# print([*failed_jobs(), *running_jobs()])\n",
    "# print(rerun_targets())\n",
    "print(job_args([14,15,62,63]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all-in-one: `subbundle3`, `subbundle4`, and `subbundle5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Adjacency Spectral Emdedding (MASE)** \n",
    "\n",
    "- UNWARPED **FA Coefficient of Determination (FA R2)** \n",
    "\n",
    "- UNWARPED **MD Coefficient of Determination (MD R2)** \n",
    "\n",
    "- **Inverse Scaled Minimum average Direct-Flip (IS MDF) distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subbundle(subject, session, bundle_name, n_clusters, clean_bundles=True):\n",
    "    \"\"\"\n",
    "    Run clustering for K=`n_clusters` on `subject`, `session`, `bundle_name` \n",
    "    using `clean_bundles` CSD tractography\n",
    "    \n",
    "    Features:\n",
    "    - Tissue - Fractional Anisotropy Coefficient of Determination (FA R^2) \n",
    "    - Distance - Inverse Scaled Minimum average Direct-Flip distance (IS_MDF)\n",
    "    \n",
    "    Generates:\n",
    "    - Adjacencey and the corresponding Laplacian matricies for each feature\n",
    "      - Joint graph embeddings using `MASE`/`OMNI` both feature matricies\n",
    "        - Clusters using both `KMeansCluster`/`GassianCluster` for each graph embedding\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import s3fs\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from dipy.io.streamline import load_tractogram, save_tractogram\n",
    "    from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "    from dipy.io.utils import create_nifti_header, get_reference_info\n",
    "    from dipy.tracking.streamline import set_number_of_points, values_from_volume, bundles_distances_mdf\n",
    "    import dipy.tracking.utils as dtu\n",
    "    import nibabel as nib\n",
    "    from graspologic.embed import MultipleASE as MASE\n",
    "    from graspologic.embed import OmnibusEmbed as OMNI\n",
    "    from graspologic.cluster import KMeansCluster, GaussianCluster\n",
    "    from graspologic.utils import to_laplace\n",
    "#     from networkx.convert_matrix import from_numpy_matrix\n",
    "#     from networkx.linalg.laplacianmatrix import laplacian_matrix\n",
    "    \n",
    "    def coeff_of_determination(data, model, axis=-1):\n",
    "        \"\"\"\n",
    "         http://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "                  _                                            _\n",
    "                 |    sum of the squared residuals              |\n",
    "        R^2 =    |1 - ---------------------------------------   | * 100\n",
    "                 |_    sum of the squared mean-subtracted data _|\n",
    "        \"\"\"\n",
    "        X = np.empty((data.shape[0], model.shape[0]))\n",
    "        demeaned_data = data - np.mean(data, axis=axis)[...,np.newaxis]\n",
    "        ss_tot = np.sum(demeaned_data **2, axis=axis)\n",
    "        # Don't divide by 0:\n",
    "        if np.all(ss_tot==0.0):\n",
    "            X[:, :] = np.nan\n",
    "            return X\n",
    "        for ii in range(X.shape[0]):\n",
    "            for jj in range(X.shape[1]):\n",
    "                # There's no point in doing any of this: \n",
    "                if np.all(data[ii]==0.0) and np.all(model[ii]==0.0):\n",
    "                    X[ii, jj] = np.nan\n",
    "                else:\n",
    "                    residuals = data[ii] - model[jj]\n",
    "                    ss_err = np.sum(residuals ** 2, axis=axis)\n",
    "                    X[ii, jj] = 1 - (ss_err/ss_tot[ii])\n",
    "        return X\n",
    "\n",
    "    def rrss(y, yhat):        \n",
    "        residuals = y - yhat\n",
    "        rss = np.dot(residuals.T, residuals)\n",
    "        rrss = np.sqrt(rss)\n",
    "        \n",
    "        return rrss\n",
    "\n",
    "    def relabel_clusters(cluster_labels):\n",
    "        from_values = np.flip(np.argsort(np.bincount(cluster_labels))[-(np.unique(cluster_labels).size):])\n",
    "        to_values = np.arange(from_values.size)\n",
    "\n",
    "        d = dict(zip(from_values, to_values))\n",
    "\n",
    "        new_cluster_labels = np.copy(cluster_labels)\n",
    "\n",
    "        for k, v in d.items():\n",
    "            new_cluster_labels[cluster_labels == k] = v\n",
    "\n",
    "        return new_cluster_labels\n",
    "        \n",
    "    def density_map(tractogram):\n",
    "        affine, vol_dims, voxel_sizes, voxel_order = get_reference_info(tractogram)\n",
    "        tractogram_density = dtu.density_map(tractogram.streamlines, np.eye(4), vol_dims)\n",
    "        tractogram_density = np.uint8(tractogram_density)\n",
    "        nifti_header = create_nifti_header(affine, vol_dims, voxel_sizes)\n",
    "        density_map_img = nib.Nifti1Image(tractogram_density, affine, nifti_header)\n",
    "        \n",
    "        return density_map_img\n",
    "        \n",
    "    def save_cluster_tractograms_and_density_maps(bundle_tractogram, model_prefix, cluster_labels):        \n",
    "        for cluster_label in np.unique(cluster_labels):\n",
    "            # tractogram\n",
    "            f_name = f'{model_prefix}_cluster_{cluster_label}.trk'\n",
    "            cluster_indicies = np.array(np.where(cluster_labels == cluster_label)[0])\n",
    "            tg = StatefulTractogram.from_sft(bundle_tractogram.streamlines[cluster_indicies], bundle_tractogram)\n",
    "            save_tractogram(tg, f_name, bbox_valid_check=False)\n",
    "            print('saving cluster tractogram:', f_name)\n",
    "            \n",
    "            # density map -- 8-bit unsigned int and gz\n",
    "            f_name = f'{model_prefix}_cluster_{cluster_label}_density_map.nii.gz'\n",
    "            tg.to_vox()\n",
    "            nib.save(density_map(tg), f_name)\n",
    "            print('saving cluster density map:', f_name)\n",
    "            \n",
    "            # TODO save cluster afq_profile\n",
    "            \n",
    "    def cluster(clusterer, embedding, model_prefix):\n",
    "        idx = clusterer.fit_predict(embedding)\n",
    "        # sort cluster by size, for convience, \n",
    "        # will need to resort by cross-subject similarity\n",
    "        idx = relabel_clusters(idx)\n",
    "        f_name = f'{model_prefix}_idx.npy'\n",
    "        np.save(f_name, idx)\n",
    "        print('saving:', f_name)\n",
    "        return idx\n",
    "            \n",
    "    def run_cluster_algos(tractogram, embedder_name, embedding, embedding_name):\n",
    "        # KMeans\n",
    "        kmeans_model_prefix = f'{embedder_name}_kmeans_{embedding_name}'\n",
    "        kmeans_clusterer = KMeansCluster(n_clusters)\n",
    "        kmeans_idx = cluster(kmeans_clusterer, embedding, kmeans_model_prefix)\n",
    "        print(f'{embedder_name} kmeans silhoutte', kmeans_clusterer.silhouette_)\n",
    "        save_cluster_tractograms_and_density_maps(tractogram, kmeans_model_prefix, kmeans_idx)\n",
    "    \n",
    "        # GMM\n",
    "        gmm_model_prefix = f'{embedder_name}_gmm_{embedding_name}'\n",
    "        gmm_clusterer = GaussianCluster(max_components=n_clusters)\n",
    "        gmm_idx = cluster(gmm_clusterer, embedding, 'mase_gmm_fa_r2_is_mdf')\n",
    "        print(f'{embedder_name} gmm bic', gmm_clusterer.bic_)    \n",
    "        save_cluster_tractograms_and_density_maps(tractogram, gmm_model_prefix, gmm_idx)\n",
    "        \n",
    "    def run_embeddings(tractogram, features, feature_name):\n",
    "        ### multiple adjacency spectral embedding (mase) ###\n",
    "        print('mase')\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        embedder = MASE()\n",
    "        embedder_name = 'mase'\n",
    "        embedding = embedder.fit_transform(features)\n",
    "        print(embedder_name, feature_name, embedding.shape)\n",
    "        run_cluster_algos(tractogram, embedder_name, embedding, feature_name)\n",
    "\n",
    "        toc = time.perf_counter()\n",
    "        print(f'mase {toc - tic:0.4f} seconds')\n",
    "\n",
    "        ### ombnibus embedding for multiple graphs (OMNI) ###\n",
    "        print('omni')\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        embedder = OMNI()\n",
    "        embedder_name = 'omni'\n",
    "        embedding = embedder.fit_transform(features)\n",
    "        print(embedder_name, feature_name, embedding.shape)\n",
    "        print((embedding[0]==embedding[1]).all())\n",
    "        run_cluster_algos(tractogram, embedder_name, embedding, feature_name)\n",
    "\n",
    "        toc = time.perf_counter()\n",
    "        print(f'omni {toc - tic:0.4f} seconds')\n",
    "\n",
    "    print(\"begin\", subject, session, bundle_name, n_clusters)\n",
    "    \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    ### fractional anisotropy scalar file ###\n",
    "    fa_scalar_filename = 'FA.nii.gz'\n",
    "    print('loading FA scalar file:', fa_scalar_filename)\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    fs.get(\n",
    "        (\n",
    "            f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "            f'{session.lower()}_afq_CSD/sub-{subject}/ses-01/'\n",
    "            f'sub-{subject}_dwi_model-DTI_FA.nii.gz'\n",
    "        ),\n",
    "        f'{fa_scalar_filename}'\n",
    "    )\n",
    "    \n",
    "    fa_scalar_data = nib.load(fa_scalar_filename).get_fdata()\n",
    "    toc = time.perf_counter()\n",
    "    print(f'scalar file: {toc - tic:0.4f} seconds')\n",
    "    \n",
    "    ### single shell deterministic bundle tractography ###\n",
    "    tractogram_filename = f'{bundle_name}.trk'\n",
    "    print('loading tractogram:', tractogram_filename)\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    bundle_folder = 'bundles'\n",
    "    \n",
    "    if clean_bundles:\n",
    "        bundle_folder = 'clean_' + bundle_folder\n",
    "        \n",
    "    fs.get(\n",
    "        (\n",
    "            f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "            f'{session.lower()}_afq_CSD/sub-{subject}/ses-01/'\n",
    "            f'{bundle_folder}/sub-{subject}_dwi_space-RASMM_model-CSD_desc-det-afq-{bundle_name}_tractography.trk'\n",
    "        ),\n",
    "        f'{tractogram_filename}'\n",
    "    )\n",
    "    \n",
    "    tractogram = load_tractogram(tractogram_filename, 'same')\n",
    "    toc = time.perf_counter()\n",
    "    print(f'tractogram file: {toc - tic:0.4f} seconds')\n",
    "    \n",
    "    ### streamline profiles ###\n",
    "    print('calculating streamline profiles')\n",
    "    tic = time.perf_counter()\n",
    "    n_points = 100\n",
    "    \n",
    "    fgarray = set_number_of_points(tractogram.streamlines, n_points)\n",
    "    \n",
    "    if len(fgarray) == 0:\n",
    "        return\n",
    "    \n",
    "    # FA Values\n",
    "    fa_values = np.array(values_from_volume(fa_scalar_data, fgarray, tractogram.affine))\n",
    "    f_name = 'streamline_profile_fa.npy'\n",
    "    np.save(f_name, fa_values)\n",
    "    print('saving:', f_name)\n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print('fa values:', fa_values.shape)\n",
    "    \n",
    "    print(f'streamline profile: {toc - tic:0.4f} seconds')\n",
    "    \n",
    "    ### Inverse Scaled MDF (Minimum average Direct-Flip) ###\n",
    "    print('calculating mdf')\n",
    "    tic = time.perf_counter()\n",
    "    mdf = bundles_distances_mdf(fgarray, fgarray)\n",
    "    \n",
    "    # enforce symmetry\n",
    "    mdf = (mdf + mdf.T) / 2\n",
    "    \n",
    "    # inverse scale\n",
    "    is_mdf = (mdf.max() - mdf)\n",
    "    is_mdf = is_mdf / is_mdf.max()\n",
    "\n",
    "    f_name = 'adjacency_is_mdf.npy'\n",
    "    np.save(f_name, is_mdf)\n",
    "    print('saving:', f_name)\n",
    "    toc = time.perf_counter()\n",
    "    \n",
    "    print('is_mdf:', is_mdf.shape)\n",
    "    print(f'mdf {toc - tic:0.4f} seconds')\n",
    "    \n",
    "    ### streamline r2 ###\n",
    "    print('calculating streamline r2')\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    # calculate FA R2\n",
    "    fa_r2 = coeff_of_determination(fa_values, fa_values)\n",
    "    \n",
    "    # enforce symmetry\n",
    "    fa_r2 += fa_r2.T\n",
    "    fa_r2 = fa_r2/2\n",
    "    \n",
    "    # save file\n",
    "    f_name = 'adjacency_fa_r2.npy'\n",
    "    np.save(f_name, fa_r2)\n",
    "    print('saving:', f_name)\n",
    "    \n",
    "    print('adjacency_fa_r2:', fa_r2.shape)\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    \n",
    "    print(f'streamline r2: {toc - tic:0.4f} seconds')\n",
    "    \n",
    "    ### laplacian matrices ###\n",
    "    \n",
    "    # TODO graspologic.utils.to_laplacian\n",
    "    \n",
    "#     lap_is_mdf = laplacian_matrix(from_numpy_matrix(is_mdf))\n",
    "    lap_is_mdf = to_laplace(is_mdf)\n",
    "    f_name = 'laplacian_is_mdf.npy'\n",
    "    np.save(f_name, lap_is_mdf)\n",
    "    print('saving:', f_name)\n",
    "    \n",
    "#     lap_fa_r2 = laplacian_matrix(from_numpy_matrix(fa_r2))\n",
    "    lap_fa_r2 = to_laplace(fa_r2)\n",
    "    f_name = 'laplacian_fa_r2.npy'\n",
    "    np.save(f_name, lap_fa_r2)\n",
    "    print('saving:', f_name)\n",
    "    \n",
    "    ### clustering ###\n",
    "    \n",
    "    fa_tissue = np.load('adjacency_fa_r2.npy')\n",
    "    distance = np.load('adjacency_is_mdf.npy')\n",
    "    features = [fa_tissue, distance]\n",
    "    feature_name = 'fa_r2_is_mdf'\n",
    "    run_embeddings(tractogram, features, feature_name)\n",
    "    \n",
    "    fa_tissue = np.load('laplacian_fa_r2.npy')\n",
    "    distance = np.load('laplacian_is_mdf.npy')\n",
    "    features = [fa_tissue, distance]\n",
    "    feature_name = 'lap_fa_r2_is_mdf'\n",
    "    run_embeddings(tractogram, features, feature_name)\n",
    "    \n",
    "    ### upload everything to s3 ###\n",
    "#     fs.put('*.npy', f'hcp-subbundle/{session}/{bundle_name}/{subject}/{n_clusters}')\n",
    "#     fs.put('*.nii.gz', f'hcp-subbundle/{session}/{bundle_name}/{subject}/{n_clusters}')\n",
    "#     fs.put('*.trk', f'hcp-subbundle/{session}/{bundle_name}/{subject}/{n_clusters}')\n",
    "    \n",
    "    print(\"end\", subject, session, bundle_name, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test locally before running on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin 103818 HCP_1200 SLF_L 10\n",
      "loading FA scalar file: FA.nii.gz\n",
      "scalar file: 0.8215 seconds\n",
      "loading tractogram: SLF_L.trk\n",
      "tractogram file: 0.5704 seconds\n",
      "calculating streamline profiles\n",
      "saving: streamline_profile_fa.npy\n",
      "fa values: (1245, 100)\n",
      "streamline profile: 0.1937 seconds\n",
      "calculating mdf\n",
      "saving: adjacency_is_mdf.npy\n",
      "is_mdf: (1245, 1245)\n",
      "mdf 3.1778 seconds\n",
      "calculating streamline r2\n",
      "saving: adjacency_fa_r2.npy\n",
      "adjacency_fa_r2: (1245, 1245)\n",
      "streamline r2: 25.3563 seconds\n",
      "mase\n",
      "mase fa_r2_is_mdf (1245, 3)\n",
      "saving: mase_kmeans_fa_r2_is_mdf_idx.npy\n",
      "mase kmeans silhoutte [0.3604991294014404, 0.43764661840038377, 0.3950001133060425, 0.41811485345326926, 0.39462845194762686, 0.3948060462703222, 0.3929254955707912, 0.39506711164849057, 0.374179033136655]\n",
      "saving cluster tractogram: mase_kmeans_fa_r2_is_mdf_cluster_0.trk\n",
      "saving cluster density map: mase_kmeans_fa_r2_is_mdf_cluster_0_density_map.nii.gz\n",
      "saving cluster tractogram: mase_kmeans_fa_r2_is_mdf_cluster_1.trk\n",
      "saving cluster density map: mase_kmeans_fa_r2_is_mdf_cluster_1_density_map.nii.gz\n",
      "saving cluster tractogram: mase_kmeans_fa_r2_is_mdf_cluster_2.trk\n",
      "saving cluster density map: mase_kmeans_fa_r2_is_mdf_cluster_2_density_map.nii.gz\n",
      "saving: mase_gmm_fa_r2_is_mdf_idx.npy\n",
      "mase gmm bic        spherical          diag          tied          full\n",
      "2  -18113.910155 -21730.990601 -21675.548413 -21956.578772\n",
      "3  -18871.550606 -22010.656973 -21724.003063 -22198.662288\n",
      "4  -19279.441445 -22288.188673 -21866.090720 -22493.716463\n",
      "5  -19441.253207 -22433.658394 -22350.455854 -22516.002480\n",
      "6  -19898.131659 -22429.570133 -22412.479291 -22607.901275\n",
      "7  -20207.191684 -22556.127479 -22445.582555 -22655.967453\n",
      "8  -20327.351649 -22541.969419 -22349.211148 -22645.216246\n",
      "9  -20409.298415 -22559.910995 -22440.711141 -22615.083524\n",
      "10 -20556.272927 -22565.876815 -22500.453398 -22628.490961\n",
      "None\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_0.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_0_density_map.nii.gz\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_1.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_1_density_map.nii.gz\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_2.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_2_density_map.nii.gz\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_3.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_3_density_map.nii.gz\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_4.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_4_density_map.nii.gz\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_5.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_5_density_map.nii.gz\n",
      "saving cluster tractogram: mase_gmm_fa_r2_is_mdf_cluster_6.trk\n",
      "saving cluster density map: mase_gmm_fa_r2_is_mdf_cluster_6_density_map.nii.gz\n",
      "mase 7.4964 seconds\n",
      "end 103818 HCP_1200 SLF_L 10\n"
     ]
    }
   ],
   "source": [
    "subbundle('103818', 'HCP_1200', 'SLF_L', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck\n",
    "ck.set_region('us-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconnect to existing knot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot_name = 'hcp-subbundle-2021-01-11T11-56-26'\n",
    "knot = ck.Knot(name=knot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cloudknot.dockerimage:Warning, your Dockerfile will have a base image of python:3, which may default to Python 3.8. This may cause dependency conflicts. If this build fails, consider rerunning with the `base_image='python:3.7' parameter.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "knot = ck.Knot(\n",
    "    name='hcp-subbundle-' + datetime.now().isoformat()[:-7].replace(':','-'),\n",
    "    func=subbundle,\n",
    "    base_image='python:3.8',\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    memory=32000,  # in MB\n",
    "    volume_size=50,  # in GB\n",
    "    bid_percentage=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse existing Docker Image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "di = ck.DockerImage(\n",
    "    name='subbundle',\n",
    "    base_image=\"python:3.8\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot = ck.Knot(\n",
    "    name='hcp-subbundle-2021-01-11T11-56-26',\n",
    "    docker_image=di,\n",
    "    base_image='python:3.8',\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    memory=32000,  # in MB\n",
    "    volume_size=50,  # in GB\n",
    "    bid_percentage=105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run subset as trial\n",
    "# result_futures = knot.map(args[:4], starmap=True)\n",
    "\n",
    "# run all\n",
    "result_futures = knot.map(args, starmap=True)\n",
    "\n",
    "# rerun failed\n",
    "# result_futures = knot.map(rerun_targets(), starmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "85b78010-db88-4e0f-adb8-93c807b9d451        hcp-subbundle-2021-01-27T12-12-43-1        SUBMITTED\n",
      "8e5254db-e11a-4ec1-83b2-aef04881c071        hcp-subbundle-2021-01-27T12-12-43-0        PENDING  \n"
     ]
    }
   ],
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PENDING',\n",
       " 'statusReason': None,\n",
       " 'attempts': [],\n",
       " 'arrayProperties': {'statusSummary': {'STARTING': 0,\n",
       "   'FAILED': 0,\n",
       "   'RUNNING': 0,\n",
       "   'SUCCEEDED': 0,\n",
       "   'RUNNABLE': 528,\n",
       "   'SUBMITTED': 0,\n",
       "   'PENDING': 0},\n",
       "  'size': 528}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knot.jobs[1].status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.clobber(clobber_pars=True, clobber_repo=True, clobber_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
