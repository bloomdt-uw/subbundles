{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subbundles Part 5: Clustering\n",
    "\n",
    "**Subbundle** - a subgroup of streamlines with a set of common properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyclustertend import hopkins, assess_tendency_by_metric\n",
    "\n",
    "from sklearn.cluster import MeanShift, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import scipy.cluster.hierarchy as spc\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamline Profiles (from Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HCP_retest'\n",
    "# dataset_name = 'HCP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = get_subjects(dataset_name)\n",
    "subjects = get_subjects_small(dataset_name)\n",
    "# subjects = get_subjects_medium(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myafq = get_afq(dataset_name)\n",
    "display(myafq.data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle_names = [*myafq.bundle_dict]\n",
    "# bundle_names = ['SLF_L', 'SLF_R']\n",
    "# bundle_names = ['ARC_L', 'ARC_R', 'CST_L', 'CST_R', 'FP'] \n",
    "bundle_names = ['SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'CST_L', 'CST_R', 'FP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dirs = {}\n",
    "fa_values = {}\n",
    "warped_fa_values = {}\n",
    "md_values = {}\n",
    "warped_md_values = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    target_dirs[subject] = {}\n",
    "    \n",
    "    loc = get_iloc(myafq, subject)\n",
    "\n",
    "    for bundle_name in bundle_names:\n",
    "        target_dir = get_dir_name(myafq, dataset_name, bundle_name, loc)\n",
    "        target_dirs[subject][bundle_name] = target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacencies (From Part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencies = {}\n",
    "adjacencies_names = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    adjacencies[subject] = {}\n",
    "    adjacencies_names[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        target_dir = target_dirs[subject][bundle_name]\n",
    "#         adjacencies[subject][bundle_name] = get_adjacencies(target_dir, '*wt*pairwise*')\n",
    "        adjacencies[subject][bundle_name] = get_adjacencies(target_dir)\n",
    "#         adjacencies_names[subject][bundle_name] = get_adjacencies_names(target_dir, '*wt*pairwise*')\n",
    "        adjacencies_names[subject][bundle_name] = get_adjacencies_names(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencies_df = pd.DataFrame.from_dict(\n",
    "    {(i,j): pd.Series(adjacencies_names[i][j]) for i in adjacencies_names.keys() for j in adjacencies_names[i].keys()}, \n",
    "    orient='index'\n",
    ")\n",
    "\n",
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(adjacencies_df)\n",
    "    \n",
    "# os.makedirs(op.join('subbundles', dataset_name), exist_ok=True)\n",
    "# f_name = op.join('subbundles', dataset_name, f'adjacencies_names.csv')\n",
    "# print(f_name)\n",
    "# adjacencies_df.to_csv(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cluster Tendency Metrics](https://en.wikipedia.org/wiki/Cluster_analysis#Cluster_Tendency)\n",
    "\n",
    "- [Hopkins Statistics](https://en.wikipedia.org/wiki/Hopkins_statistic)\n",
    "\n",
    "A statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process (and are thus uniformly randomly distributed)\n",
    "\n",
    "**Scores between 0 and 1, a score around 0.5 express no clusterability and a score tending to 0 express a high cluster tendency.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "            print(f'{dataset_name} {subject} {bundle_name} {name} cluster tendency:', hopkins(adjacency, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Silhouette _(using Kmeans)_](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "\n",
    "The **silhouette** is a measure of how similar an object is to its own cluster (*cohesion*) compared to other clusters (*separation*).\n",
    "\n",
    "Assumes a minimum of two clusters. \n",
    "\n",
    "**Returns the recommended number of clusters based on silhouette score, where the best score is 1 and worst is -1. Note that values near 0 indicate overlapping clusters.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "            print(f'{dataset_name} {subject} {bundle_name} {name} cluster tendency (silhouette):', assess_tendency_by_metric(adjacency, \"silhouette\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute/reorder matrix by similarity\n",
    "\n",
    "- this becomes convex optimization problem\n",
    "\n",
    "- many clustering algorithms would work for this setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dimensionality Reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
    "\n",
    "Dimensionality reduction can be thought of as a neutral clustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: There are many [dimensionality reduction techniques](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/) available to choose from.</span>\n",
    "\n",
    "- <span style=\"color:red\">**Question: Which others make sense to explore? and how to compare?**</span>\n",
    "\n",
    "\n",
    "- [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) (*Principle Component Analysis*)\n",
    "\n",
    "- ICA (*Independent Component Analysis*)\n",
    "\n",
    "- t-SNE (*t-distributed Stochastic Neighbor Embedding*)\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "##### <span style=\"color:red\">NOTE: `n_components` is hardcoded</span>\n",
    "\n",
    "- at the moment focused on SLF which believe from literature to have 3 subbundles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pcas = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    pcas[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        pcas[subject][bundle_name] = []\n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "            pca = PCA(n_components=3)\n",
    "            pca.fit(adjacency)\n",
    "            pcas[subject][bundle_name].append(pca)\n",
    "\n",
    "            plt.plot(pca.explained_variance_ratio_.cumsum(), label=name)\n",
    "            plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "            plt.ylabel('cumulative explained variance')\n",
    "            plt.xticks(np.arange(3))\n",
    "            plt.xlabel('component')\n",
    "\n",
    "        #     print(f\"{name} eigenvectors:\\n\", pca.components_.T)\n",
    "        #     print(f\"{name} eigenvalues:\\n\", pca.explained_variance_)\n",
    "        #     print(f\"{name} loadings:\\n\", pca.components_.T * np.sqrt(pca.explained_variance_))\n",
    "        #     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE that the eigenvectors are very short and hard to see in this"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#i = adjacencies_names.index('mdf')\n",
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        for name, adjacency, pca in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name], pcas[subject][bundle_name]):\n",
    "            X = pca.transform(adjacency)\n",
    "\n",
    "            fig = plt.figure(figsize=(5, 5))\n",
    "            plt.title(f'{dataset_name} {subject} {bundle_name} {name}')\n",
    "\n",
    "            ax = Axes3D(fig, rect=[0, 0, 0.75, 1], elev=45, azim=120)\n",
    "            sc = ax.scatter(X[:, 0], X[:, 1], X[:, 2], edgecolor='k')\n",
    "\n",
    "            for v in pca.components_:\n",
    "                ax.quiver(\n",
    "                    pca.mean_[0],  pca.mean_[1],  pca.mean_[2], # <-- starting point of vector\n",
    "                    v[0] - pca.mean_[0], v[1] -  pca.mean_[1], v[2] -  pca.mean_[2], # <-- directions of vector\n",
    "                    arrow_length_ratio=0.1, color='red'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING: Using clustering results from below to color code clusters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# i = adjacencies_names.index('wt')\n",
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        for name, adjacency, pca, idx in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name], pcas[subject][bundle_name], idxs[subject][bundle_name]):\n",
    "            X = pca.transform(adjacency)\n",
    "\n",
    "            fig = plt.figure(figsize=(5, 5))\n",
    "            plt.title(f'{dataset_name} {subject} {bundle_name} {name}')\n",
    "            ax = Axes3D(fig, rect=[0, 0, 0.75, 1], elev=45, azim=120)\n",
    "            sc = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=idx, edgecolor='k')r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "- For every streamline assign class given collection in unsupervised manner\n",
    "\n",
    "  - Then label clusters\n",
    "    \n",
    "    - To identify number of clusters use empirical test like F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Spectral Clustering](https://en.wikipedia.org/wiki/Spectral_clustering)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc_idxs = {}\n",
    "sc_names = {}\n",
    "sc_scores = {}\n",
    "\n",
    "sc = SpectralClustering(affinity=\"precomputed\", n_clusters=3)\n",
    "\n",
    "for subject in subjects:\n",
    "    sc_idxs[subject] = {}\n",
    "    sc_names[subject] = {}\n",
    "    sc_scores[subject] = {}\n",
    "    \n",
    "    for bundle_name in bundle_names:\n",
    "        sc_idxs[subject][bundle_name] = []\n",
    "        sc_names[subject][bundle_name] = []\n",
    "        sc_scores[subject][bundle_name] = []\n",
    "        \n",
    "        target_dir = target_dirs[subject][bundle_name]\n",
    "        \n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "            tic = time.perf_counter()\n",
    "            idx = sc.fit(np.absolute(adjacency)).labels_\n",
    "            toc = time.perf_counter()\n",
    "#             print(dataset_name, subject, bundle_name, name, f'{toc - tic:0.4f} seconds')\n",
    "            \n",
    "            sc_idxs[subject][bundle_name].append(idx)\n",
    "            \n",
    "            f_name = op.join(target_dir, f'sc_{name}_idx.npy')\n",
    "            sc_names[subject][bundle_name].append(name)\n",
    "            np.save(f_name, idx)\n",
    "    \n",
    "            if len(np.unique(idx)) > 1:\n",
    "                sc_scores[subject][bundle_name].append(silhouette_score(adjacency, idx))\n",
    "#             if len(np.unique(idx)) > 1:\n",
    "#                 print(f'{dataset_name} {subject} {bundle_name} {name} silhouette score:', silhouette_score(adjacency, idx))\n",
    "#             else:\n",
    "#                 print(f'{dataset_name} {subject} {bundle_name} {name} single cluster!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data = [[subject, bundle_name, name, score] for subject in sc_names.keys() for bundle_name in sc_names[subject].keys() for name, score in zip(sc_names[subject][bundle_name], sc_scores[subject][bundle_name])]\n",
    "sc_df = pd.DataFrame(sc_data, columns=['subject', 'bundle_name', 'name', 'score'])\n",
    "\n",
    "# display(sc_df)\n",
    "\n",
    "for subject in sc_names.keys():\n",
    "    for bundle_name in sc_names[subject].keys():\n",
    "        sc_df.loc[(sc_df['subject']==subject)  & (sc_df['bundle_name']==bundle_name)].plot(figsize=(10,5), title=f'{subject} {bundle_name} spectral clustering silhouette scores', use_index=False) \n",
    "        xlabels = sc_df.loc[(sc_df['subject']==subject)  & (sc_df['bundle_name']==bundle_name)].name\n",
    "        plt.xticks(range(0,len(xlabels)), xlabels, rotation='vertical')\n",
    "        f_name = op.join('subbundles', dataset_name, f'sc_{bundle_name}_silhouette_scores.png')\n",
    "        print(f_name)\n",
    "        plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "        \n",
    "# os.makedirs(op.join('subbundles', dataset_name), exist_ok=True)\n",
    "# f_name = op.join('subbundles', dataset_name, f'sc_names.csv')\n",
    "# print(f_name)\n",
    "# sc_names_df.to_csv(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution model: Gaussian mixture models (GMM)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/mixture.html#mixture"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "            n_components = np.arange(1, 11)\n",
    "            models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(adjacency) for n in n_components]\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.title('{dataset_name} {subject} {bundle_name} {name} GMM AIC/BIC Comparison')\n",
    "            plt.plot(n_components, [m.bic(adjacency) for m in models], label='BIC')\n",
    "            plt.plot(n_components, [m.aic(adjacency) for m in models], label='AIC')\n",
    "            plt.legend(loc='best')\n",
    "            plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: Despite BIC and AIC results manually setting `n_components=3` to the desired number of clusters.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_idxs = {}\n",
    "gmm_names = {}\n",
    "gmm_scores = {}\n",
    "\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "\n",
    "for subject in subjects:\n",
    "    gmm_idxs[subject] = {}\n",
    "    gmm_names[subject] = {}\n",
    "    gmm_scores[subject] = {}\n",
    "    \n",
    "    for bundle_name in bundle_names:\n",
    "        gmm_idxs[subject][bundle_name] = []\n",
    "        gmm_names[subject][bundle_name] = []\n",
    "        gmm_scores[subject][bundle_name] = []\n",
    "        \n",
    "        target_dir = target_dirs[subject][bundle_name]\n",
    "        \n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "\n",
    "            tic = time.perf_counter()\n",
    "            gmm.fit(adjacency)\n",
    "            idx = gmm.predict(adjacency)\n",
    "            toc = time.perf_counter()\n",
    "            print(dataset_name, subject, bundle_name, name, f'{toc - tic:0.4f} seconds')\n",
    "            gmm_idxs[subject][bundle_name].append(idx)\n",
    "\n",
    "            f_name = op.join(target_dir, f'gmm_{name}_idx.npy')\n",
    "            gmm_names[subject][bundle_name].append(name)\n",
    "            np.save(f_name, idx)\n",
    "#             print(dataset_name, subject, bundle_name, name, 'saving', f_name)\n",
    "            \n",
    "            if len(np.unique(idx)) > 1:\n",
    "                gmm_scores[subject][bundle_name].append(silhouette_score(adjacency, idx))\n",
    "#             print(f'{dataset_name} {subject} {bundle_name} {name} silhouette score:', silhouette_score(adjacency, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_data = [[subject, bundle_name, name, score] for subject in gmm_names.keys() for bundle_name in gmm_names[subject].keys() for name, score in zip(gmm_names[subject][bundle_name], gmm_scores[subject][bundle_name])]\n",
    "gmm_df = pd.DataFrame(sc_data, columns=['subject', 'bundle_name', 'name', 'score'])\n",
    "\n",
    "# display(sc_df)\n",
    "\n",
    "for subject in sc_names.keys():\n",
    "    for bundle_name in gmm_names[subject].keys():\n",
    "        gmm_df.loc[(gmm_df['subject']==subject)  & (gmm_df['bundle_name']==bundle_name)].plot(figsize=(10,5), title=f'{subject} {bundle_name} gaussian mixture model silhouette scores', use_index=False) \n",
    "        xlabels = gmm_df.loc[(gmm_df['subject']==subject)  & (gmm_df['bundle_name']==bundle_name)].name\n",
    "        plt.xticks(range(0,len(xlabels)), xlabels, rotation='vertical')\n",
    "        f_name = op.join('subbundles', dataset_name, f'gmm_{bundle_name}_silhouette_scores.png')\n",
    "        print(f_name)\n",
    "        plt.savefig(f_name, bbox_inches = \"tight\")\n",
    "        \n",
    "# os.makedirs(op.join('subbundles', dataset_name), exist_ok=True)\n",
    "# f_name = op.join('subbundles', dataset_name, f'sc_names.csv')\n",
    "# print(f_name)\n",
    "# sc_names_df.to_csv(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid Model: MeanShift\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: There is a `bandwidth` parameter taking default, so bandwidth is estimated using `sklearn.cluster.estimate_bandwidth`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_idxs = {}\n",
    "ms_names = {}\n",
    "ms_scores = {}\n",
    "\n",
    "ms = MeanShift()\n",
    "\n",
    "for subject in subjects:\n",
    "    ms_idxs[subject] = {}\n",
    "    ms_names[subject] = {}\n",
    "    ms_scores[subject] = {}\n",
    "    \n",
    "    for bundle_name in bundle_names:\n",
    "        ms_idxs[subject][bundle_name] = []\n",
    "        ms_names[subject][bundle_name] = []\n",
    "        ms_scores[subject][bundle_name] = []\n",
    "        \n",
    "        target_dir = target_dirs[subject][bundle_name]\n",
    "        \n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "\n",
    "            tic = time.perf_counter()\n",
    "            idx = ms.fit(adjacency).labels_\n",
    "            toc = time.perf_counter()\n",
    "            print(dataset_name, subject, bundle_name, name, f'{toc - tic:0.4f} seconds')\n",
    "            ms_idxs[subject][bundle_name].append(idx)\n",
    "            \n",
    "            f_name = op.join(target_dir, f'ms_{name}_idx.npy')\n",
    "            ms_names[subject][bundle_name].append(name)\n",
    "            np.save(f_name, idx)\n",
    "#             print(dataset_name, subject, bundle_name, name, 'saving', f_name)\n",
    "    \n",
    "            if len(np.unique(idx)) > 1:\n",
    "                ms_scores[subject][bundle_name].append(silhouette_score(adjacency, idx))\n",
    "#             if len(np.unique(idx)) > 1:\n",
    "#                 print(f'{dataset_name} {subject} {bundle_name} {name} silhouette score:', silhouette_score(adjacency, idx))\n",
    "#             else:\n",
    "#                 print(f'{dataset_name} {subject} {bundle_name} {name} single cluster!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_data = [[subject, bundle_name, name, score] for subject in ms_names.keys() for bundle_name in ms_names[subject].keys() for name, score in zip(ms_names[subject][bundle_name], ms_scores[subject][bundle_name])]\n",
    "ms_df = pd.DataFrame(ms_data, columns=['subject', 'bundle_name', 'name', 'score'])\n",
    "\n",
    "# display(sc_df)\n",
    "\n",
    "for subject in sc_names.keys():\n",
    "    for bundle_name in ms_names[subject].keys():\n",
    "        ms_df.loc[(ms_df['subject']==subject)  & (ms_df['bundle_name']==bundle_name)].plot(figsize=(10,5), title=f'{subject} {bundle_name} mean shift silhouette scores', use_index=False) \n",
    "        xlabels = ms_df.loc[(ms_df['subject']==subject)  & (ms_df['bundle_name']==bundle_name)].name\n",
    "        plt.xticks(range(0,len(xlabels)), xlabels, rotation='vertical')\n",
    "        f_name = op.join('subbundles', dataset_name, f'ms_{bundle_name}_silhouette_scores.png')\n",
    "        print(f_name)\n",
    "        plt.savefig(f_name, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connectivity Model: Hierarchical Clusters\n",
    "\n",
    "Following example from:\n",
    "\n",
    "https://stackoverflow.com/questions/52787431/create-clusters-using-correlation-matrix-in-python/52787518#52787518\n",
    "\n",
    "Using [`scipy.cluster` Hierarchical Clustering package](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy)\n",
    "\n",
    "- see https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: The dendrogram can be used to manually tune the threshold to desired number of clusters.</span>\n",
    "\n",
    "- Here setting `p` level to `1` since expect SLF bundle to have 2 or 3 subbundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdists = {}\n",
    "linkages = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    pdists[subject] = {}\n",
    "    linkages[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        pdists[subject][bundle_name] = []\n",
    "        linkages[subject][bundle_name] = []\n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "\n",
    "            # Pairwise distances between observations in n-dimensional space\n",
    "            pdist = spc.distance.pdist(adjacency)\n",
    "            pdists[subject][bundle_name].append(pdist)\n",
    "\n",
    "            # Perform hierarchical clustering\n",
    "            linkage = spc.linkage(pdist, method='complete')\n",
    "            linkages[subject][bundle_name].append(linkage)\n",
    "\n",
    "        #     plt.figure()\n",
    "        #     plt.title(f'{dataset_name} {subject} {bundle_name} {name} dendrogram')\n",
    "        #     dendrogram(linkage, p=1, truncate_mode='level', show_leaf_counts=True)\n",
    "        #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">NOTE: Manually setting threshold based on dendrogram</span>\n",
    "\n",
    "- However, it would be better to learn threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_idxs = {}\n",
    "hier_names = {}\n",
    "hier_scores = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    hier_idxs[subject] = {}\n",
    "    hier_names[subject] = {}\n",
    "    hier_scores[subject] = {}\n",
    "    \n",
    "    for bundle_name in bundle_names:\n",
    "        hier_idxs[subject][bundle_name] = []\n",
    "        hier_names[subject][bundle_name] = []\n",
    "        hier_scores[subject][bundle_name] = []\n",
    "        \n",
    "        target_dir = target_dirs[subject][bundle_name]\n",
    "        \n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "\n",
    "            i = adjacencies_names[subject][bundle_name].index(name)\n",
    "\n",
    "            # Threshold to apply when forming flat clusters\n",
    "        #     cluster_threshold = ratios[i] * pdists[i].max()\n",
    "\n",
    "            # Forms flat clusters from the hierarchical clustering defined by the given linkage matrix\n",
    "            # so that the original observations in each flat cluster have no greater a cophenetic distance \n",
    "            # than cluster_threshold\n",
    "        #     idx = spc.fcluster(linkages[i], cluster_threshold, 'distance')\n",
    "\n",
    "            tic = time.perf_counter()\n",
    "            idx = spc.fcluster(linkages[subject][bundle_name][i], t=3, criterion='maxclust')\n",
    "            toc = time.perf_counter()\n",
    "            print(dataset_name, subject, bundle_name, name, f'{toc - tic:0.4f} seconds')\n",
    "            hier_idxs[subject][bundle_name].append(idx)\n",
    "            \n",
    "            f_name = op.join(target_dir, f'hier_{name}_idx.npy')\n",
    "            hier_names[subject][bundle_name].append(name)\n",
    "            np.save(f_name, idx)\n",
    "#             print(dataset_name, subject, bundle_name, name, 'saving', f_name)\n",
    "\n",
    "            if len(np.unique(idx)) > 1:\n",
    "                hier_scores[subject][bundle_name].append(silhouette_score(adjacency, idx))\n",
    "#             print(f'{dataset_name} {subject} {bundle_name} {name} silhouette score:', silhouette_score(adjacency, idx))\n",
    "\n",
    "        #     print(f\"{name} number of clusters:\", len(np.unique(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_data = [[subject, bundle_name, name, score] for subject in hier_names.keys() for bundle_name in hier_names[subject].keys() for name, score in zip(hier_names[subject][bundle_name], hier_scores[subject][bundle_name])]\n",
    "hier_df = pd.DataFrame(hier_data, columns=['subject', 'bundle_name', 'name', 'score'])\n",
    "\n",
    "# display(sc_df)\n",
    "\n",
    "for subject in hier_names.keys():\n",
    "    for bundle_name in hier_names[subject].keys():\n",
    "        hier_df.loc[(hier_df['subject']==subject)  & (hier_df['bundle_name']==bundle_name)].plot(figsize=(10,5), title=f'{subject} {bundle_name} hierarchical clustering silhouette scores', use_index=False) \n",
    "        xlabels = hier_df.loc[(hier_df['subject']==subject)  & (hier_df['bundle_name']==bundle_name)].name\n",
    "        plt.xticks(range(0,len(xlabels)), xlabels, rotation='vertical')\n",
    "        f_name = op.join('subbundles', dataset_name, f'hier_{bundle_name}_silhouette_scores.png')\n",
    "        print(f_name)\n",
    "        plt.savefig(f_name, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resort the matrix based on cluster\n",
    "\n",
    "This is just a visualization of clusters. It is meant as a baselines sanity check for clustering, and does not save output. Additional visualization and statistics are caluclated in part 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resort_cluster_ids(idx):\n",
    "    from_values = np.flip(np.argsort(np.bincount(idx))[-(np.unique(idx).size):])\n",
    "    to_values = np.arange(from_values.size)\n",
    "    d = dict(zip(from_values, to_values))\n",
    "    new_idx = np.copy(idx)\n",
    "    for k, v in d.items(): new_idx[idx==k] = v\n",
    "    return new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load adjacencies into dataframes to simplify sorting\n",
    "dfs = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    dfs[subject] = {}\n",
    "    for bundle_name in bundle_names:\n",
    "        dfs[subject][bundle_name] = []\n",
    "        for adjacency in adjacencies[subject][bundle_name]:\n",
    "            dfs[subject][bundle_name].append(pd.DataFrame.from_records(adjacency))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model_names = ['sc']\n",
    "model_names = ['sc', 'gmm', 'ms', 'hier']\n",
    "\n",
    "sc_idxs = {}\n",
    "gmm_idxs = {}\n",
    "ms_idxs = {}\n",
    "hier_idxs = {}\n",
    "model_idxs = [sc_idxs, gmm_idxs, ms_idxs, hier_idxs]\n",
    "\n",
    "for subject in subjects:\n",
    "    sc_idxs[subject] = {}\n",
    "    gmm_idxs[subject] = {}\n",
    "    ms_idxs[subject] = {}\n",
    "    hier_idxs[subject] = {}\n",
    "    \n",
    "    for bundle_name in bundle_names:\n",
    "        sc_idxs[subject][bundle_name] = []\n",
    "        gmm_idxs[subject][bundle_name] = []\n",
    "        ms_idxs[subject][bundle_name] = []\n",
    "        hier_idxs[subject][bundle_name] = []\n",
    "\n",
    "#         target_dir = target_dirs[subject][bundle_name]\n",
    "#         adjacencies_names = ['fa', 'r2', 'md', 'wt', 'mdf']\n",
    "#         adjacencies_names = get_adjacencies_names(target_dir, '*wt*pairwise*')\n",
    "#         adjacencies_names = get_adjacencies_names(target_dir)\n",
    "\n",
    "        for adjacency_name in adjacencies_names[subject][bundle_name]:\n",
    "            for model_name, idxs in zip(model_names, model_idxs):\n",
    "                label = f'{model_name}_{adjacency_name}'\n",
    "                idx = np.load(op.join(target_dir, f'{label}_idx.npy'))\n",
    "                idxs[subject][bundle_name].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idxs = [sc_idxs, gmm_idxs, ms_idxs, hier_idxs]\n",
    "# model_idxs = [sc_idxs]\n",
    "model_names = ['SC', 'GMM', 'MS', 'Hier']\n",
    "# model_names = ['SC']\n",
    "\n",
    "for subject in subjects:\n",
    "    for bundle_name in bundle_names:\n",
    "        target_dir = target_dirs[subject][bundle_name]\n",
    "        \n",
    "        for name, adjacency in zip(adjacencies_names[subject][bundle_name], adjacencies[subject][bundle_name]):\n",
    "            print(subject, bundle_name, name)\n",
    "            i = adjacencies_names[subject][bundle_name].index(name)\n",
    "\n",
    "            show_original = True\n",
    "            \n",
    "            # should be same as output from notebook 4, outputing for comparision convienence\n",
    "            if show_original:\n",
    "                plt.figure()\n",
    "                plt.title(f'{dataset_name} {subject} {bundle_name} {name} original')\n",
    "                plt.imshow(adjacency, cmap='hot', interpolation='nearest')\n",
    "                plt.xlabel('streamline index')\n",
    "                plt.ylabel('streamline index')\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "\n",
    "            for model_name, model_idx in zip(model_names, model_idxs):\n",
    "                # TODO should show cluster labels, not always clear where delineated\n",
    "                \n",
    "                # relabel clusters so largest cluster is first and sort\n",
    "\n",
    "                columns = [dfs[subject][bundle_name][i].columns.tolist()[j] for j in list((np.argsort(resort_cluster_ids(model_idx[subject][bundle_name][i]))))]\n",
    "#                 columns = [dfs[subject][bundle_name][i].columns.tolist()[j] for j in list((np.argsort(model_idx[subject][bundle_name][i])))]\n",
    "\n",
    "                clust_df = dfs[subject][bundle_name][i].reindex(columns, axis=1)\n",
    "#                 display(clust_df)\n",
    "                \n",
    "                rows = [dfs[subject][bundle_name][i].T.columns.tolist()[j] for j in list((np.argsort(resort_cluster_ids(model_idx[subject][bundle_name][i]))))]\n",
    "#                 rows = [dfs[subject][bundle_name][i].T.columns.tolist()[j] for j in list((np.argsort(model_idx[subject][bundle_name][i])))]\n",
    "#                 display(rows)\n",
    "                \n",
    "                clust_df = clust_df.reindex(rows, axis=0)\n",
    "#                 display(clust_df)\n",
    "  \n",
    "                labels, counts = np.unique(resort_cluster_ids(model_idx[subject][bundle_name][i]), return_counts=True)\n",
    "                plt.figure()\n",
    "                plt.title(f'{dataset_name} {subject} {bundle_name} {name} {model_name} custer freq')\n",
    "                plt.bar(labels, counts, align='center')\n",
    "                plt.gca().set_xticks(labels)\n",
    "#                 ax = pd.DataFrame(resort_cluster_ids(model_idx[subject][bundle_name][i])).plot(kind='hist', legend=False, bins=len(np.unique(model_idx[subject][bundle_name][i])), title=f'{dataset_name} {subject} {bundle_name} {name} {model_name} custer freq')\n",
    "#                 for p in ax.patches:\n",
    "#                     if p.get_height() > 0:\n",
    "#                         ax.annotate(str(int(p.get_height())), (p.get_x(), p.get_height()))\n",
    "#                 plt.xticks(np.unique(model_idx[subject][bundle_name][i]))\n",
    "                plt.ylabel('streamline frequency')\n",
    "                plt.xlabel('cluster label')\n",
    "                f_name = op.join(target_dir, f'{model_name}_{name}_hist.png')\n",
    "                print(f_name)\n",
    "                plt.savefig(f_name)\n",
    "                plt.show()\n",
    "                \n",
    "                show_sorted = True\n",
    "                \n",
    "                if show_sorted:\n",
    "                    plt.figure()\n",
    "                    plt.title(f'{dataset_name} {subject} {bundle_name} {name} {model_name} sorted')\n",
    "                    plt.imshow(clust_df, cmap='hot', interpolation='nearest')\n",
    "                    plt.xlabel('streamline index')\n",
    "                    plt.ylabel('streamline index')\n",
    "                    plt.colorbar()\n",
    "                    f_name = op.join(target_dir, f'{model_name}_{name}.png')\n",
    "                    print(f_name)\n",
    "                    plt.savefig(f_name)\n",
    "                    plt.show()\n",
    "                    \n",
    "#                     try to sort clusters by size\n",
    "\n",
    "#                     display(pd.Series(resort_cluster_ids(model_idx[subject][bundle_name][i])))\n",
    "\n",
    "#                     plt.figure()\n",
    "#                     plt.title(f'{dataset_name} {subject} {bundle_name} {name} {model_name} sorted')\n",
    "#                     plt.imshow(resort_cluster_ids(model_idx[subject][bundle_name][i]), cmap='hot', interpolation='nearest')\n",
    "#                     plt.xlabel('streamline index')\n",
    "#                     plt.ylabel('streamline index')\n",
    "#                     plt.colorbar()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**TODO add test retest check**</span>\n",
    "\n",
    "Note these are going to be of different sizes and will have different streamlines, but would like to get some sense of how clusters changed.\n",
    "\n",
    "e.g., High level check: are there similar numbers (or ratio) of streamlines per cluster? Is there some way to determine if 'similar' streamlines are classified differently? Is there a way to identify these streamliens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
