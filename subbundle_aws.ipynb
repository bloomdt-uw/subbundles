{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE `662551` not in `HCP_1200`**\n",
    "\n",
    "`s3://profile-hcp-west/hcp_reliability/single_shell/hcp_1200_afq/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\n",
    "    '103818', '105923', '111312', '114823', '115320',\n",
    "    '122317', '125525', '130518', '135528', '137128',\n",
    "    '139839', '143325', '144226', '146129', '149337',\n",
    "    '149741', '151526', '158035', '169343', '172332',\n",
    "    '175439', '177746', '185442', '187547', '192439',\n",
    "    '194140', '195041', '200109', '200614', '204521',\n",
    "    '250427', '287248', '341834', '433839', '562345',\n",
    "    '599671', '601127', '627549', '660951', # '662551', \n",
    "    '783462', '859671', '861456', '877168', '917255'\n",
    "]\n",
    "session_names = ['HCP_1200', 'HCP_Retest']\n",
    "bundle_names = ['SLF_L', 'SLF_R']\n",
    "# bundle_names = ['SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'CST_L', 'CST_R']\n",
    "# n_clusters = [2, 3, 4]\n",
    "# n_clusters = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "args = list(itertools.product(subjects, session_names, bundle_names))\n",
    "# args = list(itertools.product(subjects, session_names, bundle_names, n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('125525', 'HCP_Retest', 'SLF_R'), ('111312', 'HCP_Retest', 'SLF_R'), ('122317', 'HCP_1200', 'SLF_R'), ('122317', 'HCP_Retest', 'SLF_R'), ('185442', 'HCP_Retest', 'SLF_R'), ('287248', 'HCP_Retest', 'SLF_R')]\n"
     ]
    }
   ],
   "source": [
    "def job_args(ids):\n",
    "    import numpy as np\n",
    "    \n",
    "    return list(map(tuple, np.array(args)[ids].tolist()))\n",
    "    \n",
    "# TODO use knot to determine failed and running jobs\n",
    "# may want to stop running and rerun these with larger memory and disk allocations\n",
    "def failed_jobs():\n",
    "    import numpy as np\n",
    "    # failed_job_ids = [19,22,81,101,148,149]\n",
    "    failed_job_ids = [19,22,81,101]\n",
    "#     print(len(failed_job_ids))\n",
    "    return job_args(np.array(failed_job_ids)+8)\n",
    "\n",
    "def running_jobs():\n",
    "    import numpy as np\n",
    "    running_job_ids = [1,3,13,15,35,59,83,117,119,131,137,139,141,143,147,163]\n",
    "#     print(len(running_job_ids))\n",
    "    return job_args(np.array(running_job_ids)+8)\n",
    "\n",
    "def rerun_targets():\n",
    "    import numpy as np\n",
    "    # failed twice\n",
    "    targets = list(map(tuple, np.array([*failed_jobs(), *running_jobs()])[[0,5,6,7,10,12]]))\n",
    "    return targets\n",
    "\n",
    "# print([*failed_jobs(), *running_jobs()])\n",
    "print(rerun_targets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all-in-one: `subbundle3`, `subbundle4`, and `subbundle5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subbundle(subject, session, bundle_name, n_clusters):\n",
    "    import s3fs\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from dipy.io.streamline import load_tractogram\n",
    "    from dipy.tracking.streamline import set_number_of_points, values_from_volume, bundles_distances_mdf\n",
    "    from sklearn.metrics import r2_score\n",
    "    import nibabel as nib\n",
    "    from fastdtw import fastdtw\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    from graspologic.embed import MultipleASE as MASE\n",
    "    from graspologic.cluster import KMeansCluster\n",
    "        \n",
    "    ### fractional anisotropy scalar file ###\n",
    "    \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    scalar_filename = 'FA.nii.gz'\n",
    "    \n",
    "    fs.get(\n",
    "        (\n",
    "            f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "            f'{session.lower()}_afq/sub-{subject}/ses-01/'\n",
    "            f'sub-{subject}_dwi_model-DTI_FA.nii.gz'\n",
    "        ),\n",
    "        f'{scalar_filename}'\n",
    "    )\n",
    "    \n",
    "    scalar_data = nib.load(scalar_filename).get_fdata()\n",
    "    \n",
    "    ### clean single shell deterministic bundle tractography ###\n",
    "    \n",
    "    tractogram_filename = f'{bundle_name}.trk'\n",
    "    \n",
    "    fs.get(\n",
    "        (\n",
    "            f'profile-hcp-west/hcp_reliability/single_shell/'\n",
    "            f'{session.lower()}_afq/sub-{subject}/ses-01/'\n",
    "            f'clean_bundles/sub-{subject}_dwi_space-RASMM_model-DTI_desc-det-afq-{bundle_name}_tractography.trk'\n",
    "        ),\n",
    "        f'{tractogram_filename}'\n",
    "    )\n",
    "    \n",
    "    tractogram = load_tractogram(tractogram_filename, 'same')\n",
    "    \n",
    "    ### streamline profile ###\n",
    "    \n",
    "    n_points = 100\n",
    "    \n",
    "    fgarray = set_number_of_points(tractogram.streamlines, n_points)\n",
    "    \n",
    "    if len(fgarray) == 0:\n",
    "        return\n",
    "    \n",
    "    values = np.array(values_from_volume(scalar_data, fgarray, tractogram.affine))\n",
    "    np.save('streamline_profile_fa.npy', values)\n",
    "    \n",
    "    ### pairwise warped streamline profile ###\n",
    "    \n",
    "    dtw_values = np.zeros((values.shape[0], values.shape[0], values.shape[1]))\n",
    "                \n",
    "    for i, a in enumerate(values):\n",
    "        for j, b in enumerate(values):\n",
    "            _, path = fastdtw(a,b)\n",
    "            path = np.array(path)\n",
    "            dtw_value = a[np.append(path[np.where(path[:,1][:-1] != path[:,1][1:]),0][0], len(values.T)-1)]\n",
    "            dtw_values[i,j] = dtw_value\n",
    "            \n",
    "    dtw_pairwise_warped_filename = 'streamline_profile_dtw_pairwise_warped_fa.npy'\n",
    "    np.save(dtw_pairwise_warped_filename, dtw_values)\n",
    "    \n",
    "    ### inverse scaled mdf (minimum average direct-flip) ###\n",
    "    \n",
    "    mdf = bundles_distances_mdf(fgarray, fgarray)\n",
    "    \n",
    "    # enforce symmetry\n",
    "    mdf = (mdf + mdf.T) / 2\n",
    "    \n",
    "    # inverse scale\n",
    "    is_mdf = (mdf.max() - mdf)\n",
    "    is_mdf = is_mdf / is_mdf.max()\n",
    "\n",
    "    np.save('adjacency_is_mdf.npy', is_mdf)\n",
    "    \n",
    "    ### pairwise warped fa r2 ###\n",
    "    \n",
    "    unwarped_fa_values = values\n",
    "    dtw_pairwise_warped_fa_values = dtw_values\n",
    "    \n",
    "    # To calculate a NxN for the R^2, compare each warped streamline profile \n",
    "    # to the unwarped target streamline\n",
    "    dtw_pairwise_warped_fa_r2 = pd.DataFrame(\n",
    "        [\n",
    "            [\n",
    "                r2_score(dtw_pairwise_warped_fa_values[i][j], unwarped_fa_values[i]) \n",
    "                 for j in np.ndindex(dtw_pairwise_warped_fa_values.shape[1])\n",
    "            ]\n",
    "            for i in np.ndindex(dtw_pairwise_warped_fa_values.shape[0])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # enforce symmetric\n",
    "    dtw_pairwise_warped_fa_r2 += dtw_pairwise_warped_fa_r2.T\n",
    "    dtw_pairwise_warped_fa_r2 = dtw_pairwise_warped_fa_r2/2\n",
    "    \n",
    "    np.save('adjacency_pairwise_warped_fa_r2.npy', dtw_pairwise_warped_fa_r2)\n",
    "    \n",
    "    ### weighted adjacencies ###\n",
    "    \n",
    "    alphas = np.linspace(0,10,11)/10\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        wt_dtw_pairwise_warped_fa_r2_is_mdf = alpha * dtw_pairwise_warped_fa_r2 + (1 - alpha) * is_mdf\n",
    "        np.save(\n",
    "            f'adjacency_wt_{int(alpha*10)}_pairwise_warped_fa_r2_{int((1 - alpha)*10)}_is_mdf.npy',\n",
    "            wt_dtw_pairwise_warped_fa_r2_is_mdf\n",
    "        )\n",
    "    \n",
    "    ### spectral clustering ###\n",
    "    \n",
    "    sc = SpectralClustering(affinity=\"precomputed\", n_clusters=n_clusters)\n",
    "    \n",
    "    alphas = np.linspace(0,10,11)/10\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        name = f'wt_{int(alpha*10)}_pairwise_warped_fa_r2_{int((1 - alpha)*10)}_is_mdf'\n",
    "        \n",
    "        adjacency = np.load(f'adjacency_{name}.npy')\n",
    "        \n",
    "        sc_idx = sc.fit(np.absolute(adjacency)).labels_\n",
    "    \n",
    "        np.save(f'sc_{name}_idx.npy', sc_idx)\n",
    "    \n",
    "    ### mase ###\n",
    "    \n",
    "    embedder = MASE()\n",
    "    \n",
    "    tissue = np.load('adjacency_pairwise_warped_fa_r2.npy')\n",
    "    distance = np.load('adjacency_is_mdf.npy')\n",
    "    \n",
    "    V_hat = embedder.fit_transform([tissue, distance])\n",
    "    clusterer = KMeansCluster(n_clusters)\n",
    "    mase_idx = clusterer.fit_predict(V_hat)\n",
    "    np.save('mase_pairwise_warped_fa_r2_is_mdf_idx.npy', mase_idx)\n",
    "    \n",
    "    ### upload everything to s3 ###\n",
    "    fs.put('*.npy', f'hcp-subbundle/{session}/{bundle_name}/{subject}/{n_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck\n",
    "ck.set_region('us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot = ck.Knot(name='hcp-subbundle-2021-01-11T11-56-26')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "knot = ck.Knot(\n",
    "    name='hcp-subbundle-' + datetime.now().isoformat()[:-7].replace(':','-'),\n",
    "    func=subbundle,\n",
    "    base_image='python:3.8',\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    memory=32000,  # in MB\n",
    "    volume_size=50,  # in GB\n",
    "    bid_percentage=105)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "di = ck.DockerImage(\n",
    "    name='subbundle',\n",
    "    base_image=\"python:3.8\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot = ck.Knot(\n",
    "    name='hcp-subbundle-2021-01-11T11-56-26',\n",
    "    docker_image=di,\n",
    "    base_image='python:3.8',\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    memory=32000,  # in MB\n",
    "    volume_size=50,  # in GB\n",
    "    bid_percentage=105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_futures = knot.map([*failed_jobs(), *running_jobs()], starmap=True)\n",
    "# result_futures = knot.map(args, starmap=True)\n",
    "result_futures = knot.map(rerun_targets(), starmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "d7a903f1-9f85-4ccb-8858-cba546ebcee7        hcp-subbundle-2021-01-11T11-56-26-2        PENDING  \n",
      "9d202b03-024b-4cf4-b921-cbffd71a3e60        hcp-subbundle-2021-01-11T11-56-26-0        PENDING  \n",
      "c3b85839-2d23-40da-a6db-e1773a4050c9        hcp-subbundle-2021-01-11T11-56-26-1        FAILED   \n"
     ]
    }
   ],
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'FAILED',\n",
       " 'statusReason': 'Array Child Job failed',\n",
       " 'attempts': [],\n",
       " 'arrayProperties': {'statusSummary': {'STARTING': 0,\n",
       "   'FAILED': 7,\n",
       "   'RUNNING': 0,\n",
       "   'SUCCEEDED': 13,\n",
       "   'RUNNABLE': 0,\n",
       "   'SUBMITTED': 0,\n",
       "   'PENDING': 0},\n",
       "  'size': 20}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knot.jobs[0].status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knot.clobber(clobber_pars=True, clobber_repo=True, clobber_image=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
